# AUTOGENERATED! DO NOT EDIT! File to edit: ../src/01_Clustering.ipynb.

# %% auto 0
__all__ = ['module_path', 'get_alpha_shape', 'set_colinear', 'collinear', 'get_segments', 'get_polygons_buf', 'jaccard_distance',
           'labels_filtra', 'compute_dbscan', 'adaptative_DBSCAN', 'compute_hdbscan', 'compute_OPTICS',
           'compute_Natural_cities', 'compute_AMOEBA', 'clustering', 'recursive_clustering',
           'recursive_clustering_tree', 'SSM', 'get_tree_from_clustering', 'generate_tree_clusterize_form',
           'levels_from_strings', 'get_mini_jaccars', 'level_tag', 'get_tag_level_df_labels', 'get_dics_labels',
           'get_label_clusters_df', 'mod_cid_label', 'retag_originals']

# %% ../src/01_Clustering.ipynb 2
import os
import sys
import numpy as np
import pandas as pd
import kneed
import itertools
import shapely
import random
import time
import re
from CGAL.CGAL_Alpha_shape_2 import *
from CGAL.CGAL_Kernel import Point_2
from sklearn.cluster import DBSCAN, OPTICS
from sklearn.preprocessing import StandardScaler
from shapely.geometry import LineString
from shapely.ops import polygonize, cascaded_union
from shapely.geometry import box
from shapely.geometry import Point, Polygon, MultiPolygon
from shapely.ops import polygonize_full, linemerge, unary_union
from scipy.spatial import cKDTree, Delaunay
from graph_tool.all import triangulation, label_components
from scipy.linalg import norm

import hdbscan
import graph_tool
module_path = os.path.abspath(os.path.join('..'))
if module_path not in sys.path:
    sys.path.append(module_path)
from .TreeClusters import *

# %% ../src/01_Clustering.ipynb 3
def get_alpha_shape(point_list):
    """
    Returns a polygon representing the hull of the points sample.
    
    :param list point_list: list list of tuples with samples coordinates.
    
    :returns shapely.Polygon: concave hull shapely polygon
    """
    uni_po = np.unique(point_list, axis=0)
    if len(uni_po) < 3:
        raise ValueError('Alpha Shape needs more than 3 points')
    if set_colinear(uni_po) == True:
        raise ValueError('The set of points can be colinear')

    list_of_points = [Point_2(l[0], l[1]) for l in point_list]

    a = Alpha_shape_2()

    a.make_alpha_shape(list_of_points)
    a.set_mode(REGULARIZED)
    alpha = a.find_optimal_alpha(1).next()
    a.set_alpha(alpha)

    edges = []
    for it in a.alpha_shape_edges():
        edges.append(a.segment(it))

    lines = []
    for e in edges:
        source_p = (e.source().x(), e.source().y())
        target_p = (e.target().x(), e.target().y())
        lines.append(LineString([source_p, target_p]))

    return unary_union(list(polygonize(lines)))

# %% ../src/01_Clustering.ipynb 4
def set_colinear(list_points):
    """
    Check if in the list of points any of triplet of points
    is colinear
    :param list list_points: List of shapely Points
    
    :returns bool: True if all are not colinear 
    """
    for i in itertools.combinations(list_points, 3):
        if collinear(i[0], i[1], i[2]) == False:
            return False
    return True

# %% ../src/01_Clustering.ipynb 5
def collinear(p1, p2, p3):
    """
    Check if the points are colinear 
    
    :param shapely Point p1: point to chek if is colinear
    
    :param shapely Point p2: point to chek if is colinear
    
    :param shapely Point p3: point to chek if is colinear
    
    :return bool: True if are colinear
    """
    return (p1[1]-p2[1]) * (p1[0]-p3[0]) == (p1[1]-p3[1])*(p1[0]-p2[0])

# %% ../src/01_Clustering.ipynb 8
def get_segments(points):
    """ 
    Get the segments from a delaunay triangulation
    
    :param points: Point to get Delaunay triangulation and exctract points 
    
    :return edges: 
    """
    TIN = Delaunay(points)
    # list of coordinates for each edge
    edges = []
    for tr in TIN.simplices:
        for i in range(3):
            edge_idx0 = tr[i]
            edge_idx1 = tr[(i+1) % 3]
            edges.append(LineString((Point(TIN.points[edge_idx0]),
                                    Point(TIN.points[edge_idx1]))))

    return edges

# %% ../src/01_Clustering.ipynb 10
def get_polygons_buf(lines):
    """
    Obtain the poligons from the lines
    
    :param list lines: List of lines
    
    :returns shapely polygon: the union of the union of 
    edges (Polygon or multypolygon)
    """
    linework = linemerge(lines)
    linework = unary_union(linework)
    result, _, _, _ = polygonize_full(linework)
    result = unary_union(result)
    result = result.buffer(0.0000001)
    return result

# %% ../src/01_Clustering.ipynb 12
def jaccard_distance(p1, p2):
    """
    Computes the Jaccard similarity between two polygons.
    
    param: p1 shapely Poligon 
    param: p2 shapely Poligon 
    return float Jaccard distance
    """
    intersection_area = p1.intersection(p2).area  
    #print(intersection_area)
    jacc= 1 - (intersection_area)/(p1.area + p2.area - intersection_area)
    return jacc

# %% ../src/01_Clustering.ipynb 14
def labels_filtra(point_points, multy_pol):
    """
    Labels the points in the multy_pol if no polygon contains 
    a point is label as -1
    
    :param shapely MultyPoint point_points: Points to check 
    
    :param multy_pol
    
    :returns np.array: Label array with -1 if is not contained 
    in a polygon
    """
    point_Po = [Point(i) for i in  point_points]
    labels_p=[]
    if type(multy_pol)==shapely.geometry.MultiPolygon :
        for po in point_Po:
            if multy_pol.contains(po):
                for num_pol, poly in enumerate( multy_pol):
                    if poly.contains(po):
                        labels_p.append(num_pol)
                        break
            else:
                labels_p.append(-1)
    elif type(multy_pol)==shapely.geometry.Polygon :
        for po in point_Po:
            if multy_pol.contains(po):
                labels_p.append(0)
            else:
                labels_p.append(-1)
    else:
        raise ValueError('The input is not MultiPolygon or Polygon type')   
    
    return np.array(labels_p)

# %% ../src/01_Clustering.ipynb 18
def compute_dbscan(cluster,  **kwargs):
    
    """ 
    Sklearn DBSCAN wrapper.
    
    :param cluster: a (N,2) numpy array containing the obsevations

    :returns list with numpy arrays for all the clusters obtained
    """
    eps = kwargs.get( 'eps_DBSCAN',.04)
    debugg= kwargs.get( 'debugg',False)
    min_samples= kwargs.get( 'min_samples',50)
    ret_noise = kwargs.get('return_noise', False)
    # Standarize sample
    scaler = StandardScaler()
    cluster = scaler.fit_transform(cluster)
    if debugg:
        print('epsilon distance to DBSCAN: ', eps)
        print("min_samples to DBScan: ", min_samples )
        print("Number of points to fit the DBScan: ",cluster.shape[0])

    db = DBSCAN(eps=eps, min_samples=min_samples).fit(cluster)  # Check if can be run with n_jobs = -1
    
    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    core_samples_mask[db.core_sample_indices_] = True
    labels = db.labels_
    l_unique_labels = len(set(labels)) - (1 if -1 in labels else 0)
    unique_labels = set(labels) 
    cluster = scaler.inverse_transform(cluster)
    clusters = []
    #######check that not returning the same cluster 
#     if len(unique_labels) == 1 and len(cluster) == sum(labels == 0):
#         if debugg:
#             print('Its the same set of points after clustering')
#             print('Only one cluster with the same number of points \n')
#             print('Returns the points as noise')
        
#         if ret_noise == True:
#             class_member_mask = (labels == 0)
#             return clusters, points_ret[class_member_mask]
#         else:
#             return clusters # return empty cluster
    
    ########
    if debugg:
        print('Number of clusters:' ,l_unique_labels)
    
    for l in unique_labels:
        if l != -1:
            class_member_mask = (labels == l)
            clusters.append(cluster[class_member_mask])
        elif l == -1 and debugg == True:
            class_member_mask = (labels == l)
            print("Muestras consideradas ruido: ",  sum(class_member_mask))
    
    if ret_noise == True:
        class_member_mask = (labels == -1)
        return clusters, cluster[class_member_mask]
    
    return clusters

# %% ../src/01_Clustering.ipynb 21
def adaptative_DBSCAN(points2_clusters ,
                **kwargs):
    """
    The function use the knee and average to obtain a good value for epsilon and use 
    DBSCAN to obtain the clusters
    
    :param list Points points2_clusters: Point to clusterize  
    
    :param int max_k: = (Default = len(points2_clusters)*.1)
    
    :param int  min_k: (Default =50)
    
    :param int step_k: (Default = 50)
    
    :param int leaf_size: (Default = 50)
    
    :param bool scale_points: (Default = True)
    
    :param bool debugg: (Default = False)
    
    :param bool ret_noise:  (Default = True)
    
    :returns list : list of cluster. If ret_noise = True return tuple list of cluter and noise 
    """
    max_k = kwargs.get('max_k', int(len(points2_clusters)*.1))
    max_k_percent = kwargs.get('max_k_percent', None)
    min_k = kwargs.get('min_k', 50)
    step_k = kwargs.get('step_k', 50)
    leaf_size =  kwargs.get('leaf_size',50)
    scale_points= kwargs.get('scale_points',True)
    debugg = kwargs.get('verbose',False)
    ret_noise = kwargs.get('return_noise', True)
    ###### Se tienen que hacer algunos cambios para cuando
    #  los clusters son menores a los minimos establecidos previemente
    
    ##### Establecer los minimos y maximos posibles 
    if max_k > len(points2_clusters):
        raise ValueError('The max_k value is too large for the number of points')
    
    if max_k_percent != None:
        max_k = int(len(points2_clusters)*max_k_percent)
    
    if min_k >  len(points2_clusters):
        print('The min_k value is too large for the number of points returns empty clusters')
        if ret_noise == True:
            return [] , points2_clusters
        else:
            return []
    
    if step_k > len(points2_clusters):
        raise ValueError('The step_k value is too large for the number of points')

    
    if min_k == max_k:
        print('min_k reset to obtain at least 1 value')
        min_k = max_k-1

    if scale_points ==True:
        scaler = StandardScaler()
        points_arr = scaler.fit_transform(points2_clusters)
    else:
        points_arr = points2_clusters
    
    kdt=  cKDTree(points_arr, leafsize=leaf_size)
    lits_appe_all_aver=[]
    for j in range( min_k, max_k, step_k ):
        dist_va, ind = kdt.query(points_arr, k=j, workers =-1) 
        non_zero =  dist_va[:, 1:]
        non_zero = np.ndarray.flatten(non_zero)
        non_zero = np.sort(non_zero)
        lis_aver_k=[]
        for i in range(int(non_zero.shape[0]/(j-1)) -1):
            lis_aver_k.append(np.average(non_zero[i*(j-1):(i+1)*(j-1)]))

        average_arr= np.array(lis_aver_k)
        kneedle_1_average = kneed.KneeLocator(
                range(average_arr.shape[0]),
                average_arr,
                curve="convex",## This should be the case since the values are sorted 
                direction="increasing", ## This should be the case since the values are sorted incresing
                online=True, ### To find the correct knee the false returns the first find 
        )
        epsilon= kneedle_1_average.knee_y
        min_point = kneedle_1_average.knee
        #### We take the average never the less
        
        lits_appe_all_aver.append({ 'k':j,
                    'Epsilon':epsilon,
                    'value':min_point})
    
    #### Check if the list is empty
    if len(lits_appe_all_aver) ==0:
        if debugg:
            print('DBSCAN')
            print('Using 0.6 as epsilon and 20 as Minpoints')
        db_scan= DBSCAN(eps=0.6, min_samples=20).fit(points_arr)
    else:
        df_all_average= pd.DataFrame(lits_appe_all_aver)
        max_epsi_all_average= df_all_average['Epsilon'].max()
        if debugg:
            print('Valor de epsion  : ', max_epsi_all_average)
        db_scan= DBSCAN(eps=max_epsi_all_average, min_samples=min_k).fit(points_arr)
    
    ####Get the clusters
    core_samples_mask = np.zeros_like(db_scan.labels_, dtype=bool)
    core_samples_mask[db_scan.core_sample_indices_] = True
    labels = db_scan.labels_
    unique_labels = set(labels)
    if scale_points ==True:
        points_ret = scaler.inverse_transform(points_arr)
    else:
        points_ret = points_arr
    clusters = []
    #######check that not returning the same cluster 
    if len(unique_labels) == 1 and len(points2_clusters) == sum(labels == 0):
        if debugg:
            print('Only one cluster with the same number of points \n')
            print('Returns the points as noise')
        print('Its the same set of points after clustering')
        if ret_noise == True:
            class_member_mask = (labels == 0)
            return clusters, points_ret[class_member_mask]
        else:
            return clusters # return empty cluster
    
    ########
    for l in unique_labels:
        if l != -1:
            class_member_mask = (labels == l)
            clusters.append(points_ret[class_member_mask])
        elif l == -1 and debugg == True:
            class_member_mask = (labels == l)
            print("Muestras consideradas ruido: ",  sum(class_member_mask))

    if ret_noise == True:
        class_member_mask = (labels == -1)
        return clusters, points_ret[class_member_mask]

    return clusters

# %% ../src/01_Clustering.ipynb 24
def compute_hdbscan(points2_clusters,  **kwargs):
    
    """
    HDBSCAN wrapper.
    
    :param np.array cluster: a (N,2) numpy array containing the obsevations
    
    :returns:  list with numpy arrays for all the clusters obtained
    """
    
    scale_points= kwargs.get('scale_points',True)
    debugg = kwargs.get('verbose',False)
    ret_noise = kwargs.get('return_noise', True)
    min_cluster = kwargs.get('min_cluster', 20)
    if scale_points ==True:
        scaler = StandardScaler()
        points_arr = scaler.fit_transform(points2_clusters)
    else:
        points_arr = points2_clusters

    db = hdbscan.HDBSCAN( ).fit(points_arr)
    core_samples_mask = np.full_like(db.labels_, True, dtype=bool)
    labels = db.labels_
    l_unique_labels = len(set(labels)) - (1 if -1 in labels else 0)
    unique_labels = set(labels)
    if debugg:
        print('total number of clusters: ', len(unique_labels)) 
    if scale_points ==True:
        points_ret = scaler.inverse_transform(points_arr)
    else:
        points_ret = points_arr
    clusters = []
    #######check that not returning the same cluster 
    if len(unique_labels) == 1 and len(points2_clusters) == sum(labels == 0):
        if debugg:
            print('Its the same set of points after clustering')
            print('Only one cluster with the same number of points \n')
            print('Returns the points as noise')
        
        if ret_noise == True:
            class_member_mask = (labels == 0)
            return clusters, points_ret[class_member_mask]
        else:
            return clusters # return empty cluster
    
    ########

    for l in unique_labels:
        if l != -1:
            class_member_mask = (labels == l)
            clusters.append(points_ret[class_member_mask])
        elif l == -1 and debugg == True:
            class_member_mask = (labels == l)
            print("Muestras consideradas ruido: ",  sum(class_member_mask))

    if ret_noise == True:
        class_member_mask = (labels == -1)
        return clusters, points_ret[class_member_mask]

    return clusters

# %% ../src/01_Clustering.ipynb 27
def compute_OPTICS(points2_clusters,  **kwargs):
    
    """ OPTICS wrapper.
    :param np.array cluster: a (N,2) numpy array containing the obsevations
    :returns:  list with numpy arrays for all the clusters obtained
    """

    scale_points= kwargs.get('scale_points',True)
    debugg = kwargs.get('verbose',False)
    ret_noise = kwargs.get('return_noise', True)
    min_samples= kwargs.get( 'min_samples',5)
    eps_optics = kwargs.get('eps_optics', None)
    n_jobs = kwargs.get('num_jobs',None)
    xi= kwargs.get('xi',None)
    algorithm_optics= kwargs.get('algorithm_optics','kd_tree')

    if scale_points ==True:
        scaler = StandardScaler()
        points_arr = scaler.fit_transform(points2_clusters)
    else:
        points_arr = points2_clusters


    db = OPTICS(min_samples = min_samples,eps= eps_optics, n_jobs= n_jobs).fit(points2_clusters)
    core_samples_mask = np.full_like(db.labels_, True, dtype=bool)
    labels = db.labels_
    l_unique_labels = len(set(labels)) - (1 if -1 in labels else 0)
    unique_labels = set(labels)
    if debugg:
        print('total number of clusters: ', len(unique_labels)) 
    if scale_points ==True:
        points_ret = scaler.inverse_transform(points_arr)
    else:
        points_ret = points_arr
    clusters = []
    #######check that not returning the same cluster 
    if len(unique_labels) == 1 and len(points2_clusters) == sum(labels == 0):
        if debugg:
            print('Its the same set of points after clustering')
            print('Only one cluster with the same number of points \n')
            print('Returns the points as noise')
        
        if ret_noise == True:
            class_member_mask = (labels == 0)
            return clusters, points_ret[class_member_mask]
        else:
            return clusters # return empty cluster
    
    ########
    for l in unique_labels:
        if l != -1:
            class_member_mask = (labels == l)
            clusters.append(points_ret[class_member_mask])
        elif l == -1 and debugg == True:
            class_member_mask = (labels == l)
            print("Muestras consideradas ruido: ",  sum(class_member_mask))

    if ret_noise == True:
        class_member_mask = (labels == -1)
        return clusters, points_ret[class_member_mask]

    return clusters

# %% ../src/01_Clustering.ipynb 30
def compute_Natural_cities(points2_clusters,  **kwargs):
    
    """
    Compute Natural cities clustering
    
    :param np.array points2_clusters: a (N,2) numpy array containing the obsevations
    
    :returns: list with numpy arrays for all the clusters obtained
    """
    ### The function is in acordance with the all the previus functions
    scale_points= kwargs.get('scale_points',True)
    debugg = kwargs.get('verbose',False)
    ret_noise = kwargs.get('return_noise', True)
    tail_particion = kwargs.get('return_noise', 0.55)

    if scale_points ==True:
        scaler = StandardScaler()
        points_arr = scaler.fit_transform(points2_clusters)
    else:
        points_arr = points2_clusters

    edges= get_segments(points_arr)
    lenght_av  =  np.average(np.array([i.length for i in edges ]))
    ##### HT threshold
    size_edges_len= len(edges)
    edges = [i for i in edges  if i.length < lenght_av]
    size_tail_edges = len(edges)
    if debugg:
        print('Percentage in the tail: ', size_tail_edges/size_edges_len)
    if (size_tail_edges/size_edges_len < tail_particion):
        if debugg:
            print('Not meeting the minimun tail size')
        
        if ret_noise == True:
            if debugg:
                print('Entro')
            return [], points_arr
        else:
            return [] # return empty cluster
    
    
    polygons_natural_cities=  get_polygons_buf(edges)
    if debugg:
        if type(polygons_natural_cities)==shapely.geometry.MultiPolygon:
            print('Resulting number of polygons: ', len(polygons_natural_cities))

        elif type(polygons_natural_cities)==shapely.geometry.Polygon:
            print('Only 1 polygon: ')
        else:
            print('The result is not a Polygon or Multipolygon')
    labels_points = labels_filtra(points_arr, polygons_natural_cities)
    core_samples_mask = np.full_like(labels_points, True, dtype=bool)
    l_unique_labels = len(set(labels_points)) - (1 if -1 in labels_points else 0)
    unique_labels = set(labels_points)
    
    if debugg:
        print('total number of clusters: ', len(unique_labels)) 
    #### recover
    if scale_points ==True:
        points_ret = scaler.inverse_transform(points_arr)
    else:
        points_ret = points_arr

    
    clusters = []
    #######check that not returning the same cluster 
    if len(unique_labels) == 1 and len(points2_clusters) == sum(labels_points == 0):
        if debugg:
            print('Its the same set of points after clustering')
            print('Only one cluster with the same number of points \n')
            print('Returns the points as noise')
        
        if ret_noise == True:
            class_member_mask = (labels_points == 0)
            return clusters, points_ret[class_member_mask]
        else:
            return clusters # return empty cluster
    
    ########    
    for l in unique_labels:
        if l != -1:
            class_member_mask = (labels_points == l)
            clusters.append(points_ret[class_member_mask])
        elif l == -1 and debugg == True:
            class_member_mask = (labels_points == l)
            print("Point consider noise: ",  sum(class_member_mask))

    if ret_noise == True:
        class_member_mask = (labels_points == -1)
        return clusters, points_ret[class_member_mask]

    return clusters

# %% ../src/01_Clustering.ipynb 33
def compute_AMOEBA(points_array, **kwargs):
    """The function obtains the AMOEBA algorithm on level basis
    
    :param np.array points2_clusters: a (N,2) numpy array containing the obsevations
    
    :returns: list with numpy arrays for all the clusters obtained
    """
    
    scale_points= kwargs.get('scale_points',True)
    debugg = kwargs.get('verbose',False)
    ret_noise = kwargs.get('return_noise', True)
    min_leng_clus_AMOEBA= kwargs.get('min_lenght_cluster_AMOEBA', 3)
    if scale_points ==True:
        scaler = StandardScaler()
        points_arr = scaler.fit_transform(points_array)
    else:
        points_arr = points_array
    ########
    if len(points_arr) < min_leng_clus_AMOEBA:
        clusters=[]
        noise_level= np.empty((0,2))
        if ret_noise == True:
            return clusters, noise_level
        else:
            return clusters
    
    
    gr, pos_d =triangulation(points_arr, "delaunay")
    dis_d = gr.new_edge_property("double")
    for e in gr.edges():
        dis_d[e] =  norm(pos_d[e.target()].a - pos_d[e.source()].a)
    gr.edge_properties["dis"] = dis_d
    gr.vertex_properties["pos"] = pos_d
    global_edge_mean= np.nan_to_num(gr.edge_properties['dis'].get_array().mean())
    global_edge_std = np.nan_to_num(gr.edge_properties['dis'].get_array().std() )
    
    all_remove_level =[]
    all_keep_level = []
    for vert in gr.vertices():
        local_mean= np.mean([gr.edge_properties['dis'][vo_edge]  for vo_edge in vert.out_edges()])
        tolerance = global_edge_std * (global_edge_mean/local_mean)
        rem_edg_loc = []
        keep_edg_loc = []
        for ed in vert.all_edges():
            if gr.edge_properties['dis'][ed] > tolerance + global_edge_mean:
                rem_edg_loc.append(ed)
            else:
                keep_edg_loc.append(ed)
        all_keep_level.append(keep_edg_loc)
        all_remove_level.append(rem_edg_loc)
    
    all_remove_level_flat= []
    for _list in all_remove_level:
        all_remove_level_flat += _list
    all_keep_level_flat= []
    for _list in all_keep_level:
        all_keep_level_flat += _list
    level_n = gr.new_edge_property("bool", True)
    gr.edge_properties["level_n_tolerance"] = level_n
    
    #### Probably not needed or can be reduce
    #### The edge tolerance
    for i in all_remove_level_flat:
        gr.edge_properties['level_n_tolerance'][i]= False
    for i in all_keep_level_flat:
        gr.edge_properties['level_n_tolerance'][i]= True
        
    gr.set_edge_filter(prop =  gr.edge_properties['level_n_tolerance'])
    
    
    ##### If the vertex should be keep
    gr.vertex_properties["level_n_r"] = gr.new_vertex_property("bool", False)
    for vert in gr.vertices():
        if vert.in_degree() + vert.out_degree()> 0:
            gr.vertex_properties['level_n_r'][vert]= True
        else: 
            gr.vertex_properties['level_n_r'][vert]= False
    ##  to not consider the noise points
    gr.set_vertex_filter(prop =  gr.vertex_properties['level_n_r'])
    
    ## Get the connected components
    level_n_components_arr, comp_n_hist = label_components(gr)
    gr.set_vertex_filter(None)
    
    gr.vertex_properties["compo_level_n"] = gr.new_vertex_property("int", -1)
    
    gr.set_vertex_filter(prop =  gr.vertex_properties['level_n_r'])
    
    
    compo_level_res_n = gr.new_vertex_property("int", -1)
    compo_level_res_n.a = level_n_components_arr.a
    gr.vertex_properties["compo_level_res_n"] = compo_level_res_n
    
    for vert in gr.vertices():
        # print(num)
        if vert.in_degree() + vert.out_degree()> 0:
            gr.vertex_properties['compo_level_n'][vert]= gr.vertex_properties["compo_level_res_n"][vert] 
        else: 
            # print('No edge')
            gr.vertex_properties['compo_level_n'][vert]= -1
    
    
    ####### get the points for each cluster 
    clusters_result_n= np.nan_to_num(np.unique(gr.vertex_properties['compo_level_n'].a))
    clusters=[]
    noise_level= np.empty((0,2))
    #######check that not returning the same cluster 
    if len(clusters_result_n) == 1 and len(points_array) == sum(gr.vertex_properties['compo_level_n'].a  == 0):
        if debugg:
            print('Its the same set of points after clustering')
            print('Only one cluster with the same number of points \n')
            print('Returns the points as noise')
        
        if ret_noise == True:
            class_mask = (gr.vertex_properties['compo_level_n'].a  == 0)
            return clusters, points_array[class_mask] #Empty cluster list an all the points as noise
        else:
            return clusters # return empty cluster
    
    ########

    
    
    for clas in clusters_result_n :
        if clas != -1:
            clas_mask = ( gr.vertex_properties['compo_level_n'].a == clas)
            clusters.append(points_array[clas_mask])
        else:
            clas_mask = ( gr.vertex_properties['compo_level_n'].a == clas)
            noise_level= points_array[clas_mask]
    if ret_noise == True:
        return clusters, noise_level
    return clusters
        
    

# %% ../src/01_Clustering.ipynb 37
def clustering(
            t_next_level_2,
            level=None,
            algorithm='dbscan',
            **kwargs
    ):
    """Function to get the clusters for single group by
    
    :param t_next_level_2: Dictionary with the points to compute the
            cluster
    :param level:  None Level to compute (Default None)
    
    :param str algorithm : Algorithm type is supported (Default= 'dbscan')
    
    :param int min_points_cluster:  minimun number of point to consider a cluster(Default 50)
    
    :param double eps: Epsilon parameter in the case that is needed (DBSCAN).
    
    :param bool return_noise: To return the noise (Default= True)
    
    :param bool verbose: Printing (Default= False)  
    
    :returns list t_next_level_n: A list with dictionaries with the points, 
                        the parent, and noise
    """
    verbose= kwargs.get('verbose',False)
    min_points = kwargs.get( 'min_points_cluster', 50) #### creo que se deberia quitar o poner bien
    ret_noise= kwargs.get('return_noise', True)
    eps = kwargs.get('eps',0.8)  # Epsilon value to dbscan
    min_leng_clus= kwargs.get('min_lenght_cluster', 5)
    t_next_level_n = []
    if level == None:
        level = 0

    for li_num, cluster_list_D in enumerate(t_next_level_2):
        cluster_list = cluster_list_D['points']
        cluster_list_pa = cluster_list_D['parent']
        if verbose:
            print("Size cluster list: ", len(cluster_list))
            
        for c_num, cluster in enumerate(cluster_list):
            if verbose:
                print("Size cluster: ", len(cluster))
                print('Algorithm: ', algorithm)

            if len(cluster) > min_leng_clus:
                if algorithm == 'dbscan':
                    if verbose:
                        print("Epsilon Value: ", eps)
                    tmp = compute_dbscan(cluster,
                                 eps_DBSCAN = eps,
                                 debugg=verbose,
                                  **kwargs)
                    if ret_noise:
                        noise_points = tmp[1]
                        tmp =  tmp[0]
                
               
                elif algorithm == 'hdbscan':
                    tmp = compute_hdbscan(cluster,
                                **kwargs)
                    if ret_noise:
                        noise_points = tmp[1]
                        tmp =  tmp[0]
                ##########  
                elif algorithm == 'adaptative_DBSCAN':
                    #### If the number of cluster is too small 
                    
                    tmp = adaptative_DBSCAN(cluster, **kwargs)
                    if ret_noise:
                        noise_points = tmp[1]
                        tmp =  tmp[0]

                elif algorithm == 'optics':
                    tmp = compute_OPTICS(cluster,
                                eps_OPTICS = eps,
                                **kwargs)
                    if ret_noise:
                        noise_points = tmp[1]
                        tmp =  tmp[0]
                ##########  
                elif algorithm == 'natural_cities':
                    tmp = compute_Natural_cities(cluster,
                                **kwargs)
                    if ret_noise:
                        noise_points = tmp[1]
                        tmp =  tmp[0]
                ##########  
                elif algorithm == 'amoeba':
                    tmp = compute_AMOEBA(cluster,
                                **kwargs)
                    if ret_noise:
                        noise_points = tmp[1]
                        tmp =  tmp[0]
                #########
                else:
                    raise ValueError('Algorithm must be: \n', 
                                     'dbscan, hdbscan, adaptative_DBSCAN, optics, natural_cities or amoeba')
                    # sys.exit("1")
                
                
                
                if verbose:
                    print("The number of resulting clusters is : ", len(tmp))
                if ret_noise:
                    dic_clos = {'points': tmp,
                           'parent': cluster_list_pa + '_L_'+str(level) +
                            '_l_' + str(li_num) + '_c_'+str(c_num), 
                            'noise_points':noise_points
                    }
                else:
                    dic_clos = {'points': tmp, 'parent': cluster_list_pa +
                            '_L_'+str(level) + '_l_' + str(li_num) + '_c_'+str(c_num)}
                
                t_next_level_n.append(dic_clos)
            else:
                if ret_noise:
                    dic_clos = {'points': [],
                           'parent': cluster_list_pa + '_L_'+str(level) +
                            '_l_' + str(li_num) + '_c_'+str(c_num), 
                            'noise_points':cluster
                    }
                else:
                    dic_clos = {'points': [], 'parent': cluster_list_pa +
                            '_L_'+str(level) + '_l_' + str(li_num) + '_c_'+str(c_num)}
                t_next_level_n.append(dic_clos)
    
    return t_next_level_n

# %% ../src/01_Clustering.ipynb 40
def recursive_clustering(
                this_level,  # Dictionary with Points
                to_process,  # levels to process
                cluster_tree,  # to store the clusters
                level = 0,  # current level
                **kwargs
               ):
    """
    Performs the recursive clustering.
    Calls compute_dbscan for each
    list of clusters keepen the structure and then calls itself
    until no more clusters satisfy the condition
        
    :param dict this_level: level is the current level 
    
    :param int to_process: the max level to process
    
    :param double eps: The epsilon parameter distance to pass to the needed algorithm 
    
    :param list cluster_tree : list of list to insert the levels 
    
    :param bool verbose : To print 
    
    :param double decay: In the use of dbscan the deacy parameter to reduce eps
    
    :param int min_points_cluster: The min point for each cluster to pass to algorithm
    
    :param str algorithm:  The string of the algorithm name to use
    """
    algorithm= kwargs.get('algorithm' ,'dbscan')    # Algorithm to use
    verbose= kwargs.get('verbose',False)
    min_points = kwargs.get( 'min_points_cluster', 50)
    decay = kwargs.get('decay', 0.7)
    eps = kwargs.get('eps' ,0.8)  # Epsilon distance to DBSCAN parameter
    max_k_increase = kwargs.get('max_k_increase', None)
    tmp = None

    if level == 0:
        kwargs['eps'] = eps
    else:
        kwargs['eps'] = eps  * decay

    if max_k_increase != None:
        if level == 0:
            kwargs['max_k_percent'] = 0.1
        else:
            kwargs['max_k_percent'] = kwargs['max_k_percent'] * max_k_increase
    
    cluster_result_polygons = []
    if level > to_process:
        if verbose:
            print('Done clustering')
        return
    ######## Get the clusters for the current list of points 
    all_l = clustering(
                    this_level,
                    level=level,                    
                    **kwargs
                    )
    ##########

    cluster_tree.append(all_l)
    cluster_n = 0
    for i in all_l:
        cluster_n += len(i['points'])
    if verbose:
        print('At level ', level, ' the number of lists are ',
              len(all_l), ' with ', cluster_n, 'clusters')
    level += 1
    if len(all_l) > 0:
        return recursive_clustering(all_l, 
                               to_process=to_process,
                               cluster_tree=cluster_tree,
                               level= level,
                               **kwargs
                               )
    else:
        if verbose:
            print('done clustering')
        return

# %% ../src/01_Clustering.ipynb 43
def recursive_clustering_tree(dic_points_ori, **kwargs):
    """
    Obtaing the recursive tree using a specific algorithm
    """
    levels_clustering= kwargs.get('levels_clustering',4)
    cluster_tree = []
    recursive_clustering([dic_points_ori],  # Dictionary with Points
                levels_clustering,  # levels to process
                cluster_tree,  # to store the clusters
                level=0,  # current level
                **kwargs
                )
    tree_clus= get_tree_from_clustering(cluster_tree)
    tree_from_clus= TreeClusters()
    tree_from_clus.levels_nodes = tree_clus
    tree_from_clus.root= tree_from_clus.levels_nodes[0][0]   
    return tree_from_clus

# %% ../src/01_Clustering.ipynb 45
def SSM(list_poly_c_1,list_poly_c_2 ,**kwargs):
    """
    The function calculates the Similarity Shape Measurement (SSM)
    between two clusterizations 
    
    :param: list of nodes with points and polygons 
    
    :param: list of nodes with points and polygons 
    
    :param bool verbose: To print to debugg
    
    :returns double: The similarity mesuarment
    """
    verbose= kwargs.get('verbose', False)
    ##### Get intersection 
    list_de=[]
    for i in list_poly_c_1:
        list_de.append([ i.polygon_cluster.intersection(  j.polygon_cluster ) for  j in list_poly_c_2])
    
    list_de_bool = []
    for i in list_de:
        list_de_bool.append([not j.is_empty for j in i])
    
    list_de_index = []
    #print(list_de_bool)
    for i in list_de_bool:
        if any(i):
            list_de_index.append(i.index(True))
        else:
            list_de_index.append(None)
    jacc_sim_po = []
    for num, node in enumerate(list_poly_c_1):
        ### ver eque pasa cuando se tienen 2 
        if list_de_index[num] is not None:
            node_get = list_poly_c_2[list_de_index[num]]
            poli_int = list_de[num][list_de_index[num]]
            
            ####Puntos en la interseccion
            #print(node)
            points_all = node.get_point_decendent()
            res_bool =[ poli_int.contains(p) for p in points_all] ### Como no necesito los puntos basta con esto
            card = sum(res_bool)
            ###Obtenemos jaccard 
            sim_jacc = (poli_int.area)/(node.polygon_cluster.area + node_get.polygon_cluster.area - poli_int.area)
            if verbose:
                print("jaccard: " ,sim_jacc)
                print("cardinal: " ,card )
            jacc_sim_po.append(sim_jacc* card)#####Cuando hay interseccion 
        else:
            jacc_sim_po.append(0) #### Cuando no hay
    
    arr_bool = np.array(list_de_bool)

    Q_not= []
    for col in range(arr_bool.shape[1]):
        cols_sel= arr_bool[:,col].any()
        if cols_sel ==False:
            Q_not.append(col)
    #print(Q_not)
    len_Q_not=[]
    if Q_not:
         len_Q_not =[len(list_poly_c_2[i].get_point_decendent())  for i in Q_not] 

    P_sum = sum([len(node.get_point_decendent())  for node in list_poly_c_1])
    deno =P_sum + sum(len_Q_not)
    return sum(jacc_sim_po)/deno

# %% ../src/01_Clustering.ipynb 47
def get_tree_from_clustering(cluster_tree_clusters):
    """ Returns the tree from the iterative clustering, the cluster_tree_cluster
     
     :param cluster_tree_clusters is a list of list with a dictionary that 
            should contain the points of the next level clusters, the name of the parent
            cluster of such clusters (name of the current node), and the point that 
            are consider noise. 
    
     :return A list of list that contains the nodes for each level of the tree. 
    """ 
     ##### La estructura de arbol 
    all_level_clusters =[]
    previus_level=[]
    list_len = len(cluster_tree_clusters)-1
    for level_num_clus, level_cluster in enumerate(cluster_tree_clusters):
        level_nodes=[]
        for cluster_te in level_cluster:
            node_l = NodeCluster(name= cluster_te['parent'])
            to_concat= [point_arr for point_arr in cluster_te['points']]
            if len(to_concat)>0:
                points_poly = np.concatenate(to_concat)
            else:
                points_poly = np.array([], dtype= np.float64).reshape(0,2)
               
            points_poly = np.concatenate(
                                   (points_poly,cluster_te['noise_points']),
                                   axis=0
                              )
            
            
            ####### NOt a posible cluster 
            if len(points_poly) <3:
                node_l.polygon_cluster = None
            else:
                try:
                    node_l.polygon_cluster = get_alpha_shape(points_poly)
                except:
                    print('Unable to create the polygon returning None')
                    node_l.polygon_cluster = None
                
            
            ##### Es necesario que si es el último nivel todos los puntos sean
            ## considerados como ruido pues aunque se haya hecho la clusterizacion
            #  ya no se bajo al siguiente nivel
            ## 
            if level_num_clus==list_len:
                node_l.point_cluster_noise = shapely.geometry.MultiPoint(points_poly) 
            else:
                node_l.point_cluster_noise = shapely.geometry.MultiPoint(cluster_te['noise_points']) 
            
            pos=node_l.name.rfind('_L')
            if node_l.name[:pos]=='':
                node_l.parent = None
            else:
                lis_pa=[item  for item in previus_level if item.name == node_l.name[:pos]]
                node_l.parent= lis_pa[0]
            level_nodes.append(node_l)
        all_level_clusters.append(level_nodes)
        previus_level = level_nodes
    
    return  all_level_clusters

# %% ../src/01_Clustering.ipynb 48
def generate_tree_clusterize_form(**kwargs ):
    """
    Generates all the experiment all the experiment creates the data and clusterize using the algorithm available
    
    :param levels_tree: Levels for the tree
    
    :param int per_cluster: Points per clusters
    
    :param levels_cluster:  Levels to clusterize
    
    :param bool verbose:  To print some outputs 
    
    :returns: a dictionary with all the  data frames and a dictionary 
    with the similarity measurment created
     """
    
    levels_tree= kwargs.get('tree_level', 4)
    per_cluster = kwargs.get('num_per_cluster', 200)
    levels_cluster = kwargs.get('levels_cluster', 4)
    verbose = kwargs.get('verbose', False)
    
    if verbose:
        print('generating tree')
    
    
    random.seed(int(time.time()))
    random_seed = random.randint(0,1500)
    print('Random to use: ',random_seed )
    print('With',levels_tree , ' levels' )
    tree_original= TreeClusters(levels_tree, random_seed= random_seed)
    tree_original.populate_tree(number_per_cluster=per_cluster, **kwargs)
    tree_original_points= tree_original.get_points_tree()
    X_2=np.array([[p.x,p.y] for p in tree_original_points])
    dic_points_ori={'points':[X_2], 'parent':''}
    if verbose:
        print('tree with: ', X_2.shape )
    
    while X_2.shape[0] < 2000:
        
        print('tree with too few elements to clusterize creating new tree')
        random.seed(int(time.time()))
        random_seed = random.randint(0,1500)
        print('Random to use: ',random_seed )
        tree_original= TreeClusters(levels_tree, random_seed= random_seed)
        tree_original.populate_tree(number_per_cluster=per_cluster, avoid_intersec= True)
        tree_original_points= tree_original.get_points_tree()
        X_2=np.array([[p.x,p.y] for p in tree_original_points])
        dic_points_ori={'points':[X_2], 'parent':''}
        
    
    
    if verbose:
        print('tree generated')
    
    
    if verbose:
        print('clusterize and creating the trees')
    
    tree_Natural_c = recursive_clustering_tree(dic_points_ori,
                                               levels_clustering = levels_cluster,
                                              algorithm = 'natural_cities', 
                                               **kwargs,
                                              )
    tree_DBSCAN = recursive_clustering_tree(dic_points_ori,
                                              levels_clustering = levels_cluster,
                                              algorithm = 'dbscan',
                                              **kwargs,
                                           )
    tree_HDBSCAN = recursive_clustering_tree(dic_points_ori,
                                              levels_clustering = levels_cluster,
                                              algorithm = 'hdbscan',
                                              **kwargs,
                                            )
    tree_OPTICS= recursive_clustering_tree(dic_points_ori,
                                              levels_clustering = levels_cluster,
                                              algorithm = 'optics',
                                              **kwargs,
                                            )
    tree_Adap_DBSCAN = recursive_clustering_tree(dic_points_ori,
                                              levels_clustering = levels_cluster,
                                              algorithm = 'adaptative_DBSCAN',
                                              **kwargs,
                                            )
    if verbose:
        print('DONE clusterize and creating the trees')
    ######  get the points dataframe for each tree 
    data_fram_or = tree_original.get_dataframe_recursive_node_label(func_level_nodes = levels_from_strings)
    df_Natural = tree_Natural_c.get_dataframe_recursive_node_label()
    df_DBSCAN = tree_DBSCAN.get_dataframe_recursive_node_label()
    df_HDBSCAN = tree_HDBSCAN.get_dataframe_recursive_node_label()
    df_OPTICS = tree_OPTICS.get_dataframe_recursive_node_label()
    df_Adap_DBSCAN = tree_Adap_DBSCAN.get_dataframe_recursive_node_label()
    
    df_Natural.name='Natural_C'
    df_DBSCAN.name= 'DBSCAN'
    df_HDBSCAN.name= 'HDBSCAN'
    df_OPTICS.name= 'OPTICS'
    df_Adap_DBSCAN.name = 'Adap_DBSCAN'
    
    if verbose:
        print('Original size',data_fram_or.shape )
        print('Natural size',df_Natural.shape)
        print('DBSCAN size',df_DBSCAN.shape)
        print('HDBSCAN size',df_HDBSCAN.shape)
        print('OPTICS size',df_OPTICS.shape)
        print('adaptative_DBSCAN size',df_Adap_DBSCAN.shape)
    
    ######For each dataframe  
    if verbose:
        print('get dataframe Original')
    get_tag_level_df_labels(data_fram_or, levels_cluster)
    ###Natural Cities
    if verbose:
        print('get dataframe Natural cities')
        
    dic_final_levels_Natural_c = get_dics_labels(tree_original, tree_Natural_c)
    dic_label_final_levels_Natural=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_Natural_c]
    get_tag_level_df_labels(df_Natural, levels_cluster)
    for dic in dic_label_final_levels_Natural[1:]: ## En el nivel 0 no tiene sentido
        tag_ori = dic['level_ori']
        dic_lev = dic['dict']
        retag_originals(data_fram_or,
                        df_Natural,
                        tag_ori,
                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta 
                        dic_lev)
    
    
    ### DBSCAN
    if verbose:
        print('get dataframe DBSCAN')
        
    dic_final_levels_DBSCAN = get_dics_labels(tree_original, tree_DBSCAN)
    dic_label_final_levels_DBSCAN=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_DBSCAN]
    get_tag_level_df_labels(df_DBSCAN, levels_cluster)
    for dic in dic_label_final_levels_DBSCAN[1:]: ## En el nivel 0 no tiene sentido
        tag_ori = dic['level_ori']
        dic_lev = dic['dict']
        retag_originals(data_fram_or,
                        df_DBSCAN,
                        tag_ori,
                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta 
                        dic_lev)
    
    
    ##HDBSCAN
    if verbose:
        print('get dataframe HDBSCAN')
    dic_final_levels_HDBSCAN = get_dics_labels(tree_original, tree_HDBSCAN)
    dic_label_final_levels_HDBSCAN=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_HDBSCAN]
    get_tag_level_df_labels(df_HDBSCAN, levels_cluster)
    for dic in dic_label_final_levels_HDBSCAN[1:]: ## En el nivel 0 no tiene sentido
        tag_ori = dic['level_ori']
        dic_lev = dic['dict']
        retag_originals(data_fram_or,
                        df_HDBSCAN,
                        tag_ori,
                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta 
                        dic_lev)
    #### OPTICS
    if verbose:
        print('get dataframe OPTICS')
        
    dic_final_levels_OPTICS = get_dics_labels(tree_original, tree_OPTICS)
    dic_label_final_levels_OPTICS=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_OPTICS]
    get_tag_level_df_labels(df_OPTICS, levels_cluster)
    for dic in dic_label_final_levels_OPTICS[1:]: ## En el nivel 0 no tiene sentido
        tag_ori = dic['level_ori']
        dic_lev = dic['dict']
        retag_originals(data_fram_or,
                        df_OPTICS,
                        tag_ori,
                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta 
                        dic_lev)
    ##### Adap_DBSCAN
    if verbose:
        print('get adaptative DBSCAN')
        
    dic_final_levels_Adap_DBSCAN = get_dics_labels(tree_original, tree_Adap_DBSCAN)
    dic_label_final_levels_Adap_DBSCAN=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_Adap_DBSCAN]
    get_tag_level_df_labels(df_Adap_DBSCAN, levels_cluster)
    for dic in dic_label_final_levels_Adap_DBSCAN[1:]: ## En el nivel 0 no tiene sentido
        tag_ori = dic['level_ori']
        dic_lev = dic['dict']
        retag_originals(data_fram_or,
                        df_Adap_DBSCAN,
                        tag_ori,
                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta 
                        dic_lev)
    ##############################################Get only signal  noise 
    data_fram_or_sig_noise = tree_original.get_tag_noise_signal_tree()
    df_Natural_sig_noise = tree_Natural_c.get_tag_noise_signal_tree()
    df_DBSCAN_sig_noise = tree_DBSCAN.get_tag_noise_signal_tree()
    df_HDBSCAN_sig_noise = tree_HDBSCAN.get_tag_noise_signal_tree()
    df_OPTICS_sig_noise = tree_OPTICS.get_tag_noise_signal_tree()
    df_Adap_DBSCAN_sig_noise = tree_Adap_DBSCAN.get_tag_noise_signal_tree()
    
    df_Natural_sig_noise.name='I_Natural_C'
    df_DBSCAN_sig_noise.name= 'I_DBSCAN'
    df_HDBSCAN_sig_noise.name= 'I_HDBSCAN'
    df_OPTICS_sig_noise.name= 'I_OPTICS'
    df_Adap_DBSCAN_sig_noise.name = 'I_Adap_DBSCAN'
    
    
    
    
    
    
    ######Evaluate form metric
    if verbose:
        print('Niveles: ' ,len(tree_original.levels_nodes))
        print('Nodos en el ultimo nivel: ' ,len(tree_original.levels_nodes[-1]))
        #print('tag_all : ' , data_fram_or['Final_tag'].unique())
    levels_r = range(0, levels_cluster)###
    resultado_form_metric = []
    for l in levels_r:
        
        d = { 'Level': l,
            'DBSCAN': SSM(tree_original.levels_nodes[l],
                                            tree_DBSCAN.levels_nodes[l]),
             'HDBSCAN': SSM(tree_original.levels_nodes[l],
                                             tree_HDBSCAN.levels_nodes[l]),
             'Natural': SSM(tree_original.levels_nodes[l],
                                             tree_Natural_c.levels_nodes[l]),
             'OPTICS': SSM(tree_original.levels_nodes[l],
                                            tree_OPTICS.levels_nodes[l]),
             'Adap_DBSCAN': SSM(tree_original.levels_nodes[l],
                                          tree_Adap_DBSCAN.levels_nodes[l])
             }
        resultado_form_metric.append(d)
    

    return {'Point_dataframes':{'original_retag':data_fram_or,
            'Natural_C':df_Natural ,
            'DBSCAN':df_DBSCAN,
            'HDBSCAN':df_HDBSCAN,
            'OPTICS':df_OPTICS,
            'Adap_DBSCAN':df_Adap_DBSCAN
           }, 'metric_form': resultado_form_metric,
            'Noise_signal':{
            'original_retag':data_fram_or_sig_noise,
            'Natural_C':df_Natural_sig_noise ,
            'DBSCAN':df_DBSCAN_sig_noise,
            'HDBSCAN':df_HDBSCAN_sig_noise,
            'OPTICS':df_OPTICS_sig_noise,
            'Adap_DBSCAN':df_Adap_DBSCAN_sig_noise
            }
           
           }


# %% ../src/01_Clustering.ipynb 61
def levels_from_strings(
            string_tag,
            level_str='l_',
            node_str = 'n_',
            **kwargs
            ):
    """
    Returns the levels and the node id using the expected strings 
    that identify the level id and node id
    
    :param str level_str: string for the level
    
    :param str node_str: string for the nodes
    
    
    :returns tuple (levels, nodeid): 
    """
    
    positions = [i.start() for i in re.finditer( level_str, string_tag )]
    nodeid_positions = [i.start() for i in re.finditer( node_str, string_tag )]    
    
    
    #evels = [string_tag[i+len(level_str)] for i in positions ]
    levels=[]
    for i in positions:
        if string_tag[i+len(level_str):].find(node_str) != -1:   
            levels.append(string_tag[i+len(level_str):][:string_tag[i+len(level_str):].find(node_str)-1])
        else:
            levels.append(string_tag[i+len(level_str):])
    nodeid=[]
    for i in nodeid_positions:
        if string_tag[i+len(node_str):].find(level_str) != -1:   
            nodeid.append(string_tag[i+len(node_str):][:string_tag[i+len(node_str):].find(level_str)-1])
        else:
            nodeid.append(string_tag[i+len(node_str):])

    return levels, nodeid

# %% ../src/01_Clustering.ipynb 63
def get_mini_jaccars(cluster, tree_2, level_int):
    """
    Find the most similar cluster in the tree_2 at level level_int
    returns int the index of the most similar polygon in the level
    """
    tree_2_level= tree_2.get_level(level_int)
    Jaccard_i= [jaccard_distance(cluster.polygon_cluster, j.polygon_cluster) for j in tree_2_level]  
    
    valu_min = Jaccard_i.index( min(Jaccard_i))
    return valu_min
    

# %% ../src/01_Clustering.ipynb 64
def level_tag(list_tags, level_int  ):
    """
    Tags if the are noise or signal
    """
    if len(list_tags)==0:
        return 'noise'
    try:
        return list_tags[level_int]
    except:
        return 'noise'   

# %% ../src/01_Clustering.ipynb 65
def get_tag_level_df_labels(df, levels_int ):
    """
    Get the tag for the cluster
    
    :param Pandas.DataFrame df:
    
    :param int levels_int: 
    
    :returns None:
    """
    for i in range(levels_int):
        df['level_'+ str(i) +'_cluster']= df['cluster_id'].apply(lambda l:  level_tag(l,i))

# %% ../src/01_Clustering.ipynb 66
def get_dics_labels(tree_or, tree_res, **kwargs):
    """
    Obtains a list of dictionaries to retag the original tree_tag with their 
    correspondance in the tree_res on level level_get +1
    
    :param tree_or: Original tree
    
    :param tree_res: resulting tree
    
    :param level_get: int level to get
    
    :param return list of dictionaries:  
    """
    dic_list_levels= []
    ##### If the number of levels is different there is no reason that his should work 
    min_level_to_ask = min(tree_or.get_deepth(), tree_res.get_deepth())
    
    verbose= kwargs.get
    for i in range(min_level_to_ask):
        dic_level_df = get_label_clusters_df(tree_or, tree_res, i)
        ## Eliminate the clusters with nan  
        dic_level_df.dropna(axis=0, subset=['Sim_cluster'], inplace=True)
        
        dic_lev  = dic_level_df['Sim_cluster'].to_dict()
        dic_list_levels.append({'level_ori':'level_'+str(i)+'_cluster', 'dict': dic_lev})
    return dic_list_levels

# %% ../src/01_Clustering.ipynb 67
def get_label_clusters_df(tree_1, tree_2, level_int):
    """
    Obtains the dataframe with the label 
    
    :param TreeClusters tree_1: 
    
    :param TreeClusters tree_2:
    
    :param int level_int:
    
    :reutrns Pandas.DataFrame df_level_clus: 
    """
    level_all = tree_1.get_level(level_int)
    df_level_clus = pd.DataFrame(level_all, columns=['Clusters'])
    df_level_clus['Area'] = df_level_clus['Clusters'].apply(lambda l: l.polygon_cluster.area)
    df_level_clus['Name'] = df_level_clus['Clusters'].apply(lambda l: l.name)
    
    df_level_clus['Sim_cluster'] = df_level_clus['Clusters'].apply(lambda l: get_mini_jaccars(l, tree_2,level_int)) ###### Como se hacen las clusterizaciones se debe usar el siguiente nivel
    
    #print('', df_level_clus['Sim_cluster'].dtype)
    df_level_clus= df_level_clus.sort_values(by ='Area', ascending=False)
    df_level_clus['Sim_cluster'] = (df_level_clus['Sim_cluster']
                                    .where(~df_level_clus.duplicated(subset=['Sim_cluster']), None))
    #print(df_level_clus['Sim_cluster'].dtype)
    level_2= tree_2.get_level(level_int) 
    df_level_clus['Sim_cluster_name'] =(df_level_clus['Sim_cluster']
                                         .astype('int32', errors='ignore')
                                         .replace({np.nan: ''})
                                         .apply(lambda l:  level_2[int(l)].name if l !=''  else None) )
    
    return df_level_clus

# %% ../src/01_Clustering.ipynb 68
def mod_cid_label(dic_label):
    """
    
    """
    dic_label={str(k):str(v) for k,v in dic_label.items()} 
    dic_label['noise'] = 'noise'
    return dic_label

# %% ../src/01_Clustering.ipynb 69
def retag_originals(df_fram_or ,
                    df_results,
                    tag_original,
                    tag_results,
                    dic_tag_or_res):
    """
    Retags the labels in the df_fram_or using the dictionary dic_tag_or_res to match 
    the tags with the corresponding tag in the df_result and all the labels that are
    not in the dictionary generate a new tag fo them. 
    
    :param Pandas.DataFrame df_fram_or
    
    :param Pandas.DataFrame df_results
    
    :param tag_original
    
    :param tag_results
    
    :param Pandas.DataFrame dic_tag_or_res
    
    """
    tag_plus=  len(df_results[tag_results].unique()) +100  - len(df_results[tag_results].unique())%100
    df_fram_or['re_tag_'+str(df_results.name)+'_'+tag_original] = df_fram_or[tag_original].apply(lambda l: dic_tag_or_res[l] if l in dic_tag_or_res.keys() else   str(int(l) +tag_plus) )
