{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ededd5fa-4ea5-4055-beb5-2ead0f1cbdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e1d6b6-ecaa-40aa-af5e-9952e137fcc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4363959e-b31e-4325-9f9f-8d60ccbc4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kneed\n",
    "import itertools\n",
    "import shapely\n",
    "import random\n",
    "from CGAL.CGAL_Alpha_shape_2 import *\n",
    "from CGAL.CGAL_Kernel import Point_2\n",
    "from sklearn.cluster import DBSCAN, OPTICS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from shapely.geometry import LineString\n",
    "from shapely.ops import polygonize, cascaded_union\n",
    "from shapely.geometry import box\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "from shapely.ops import polygonize_full, linemerge, unary_union\n",
    "from scipy.spatial import cKDTree, Delaunay\n",
    "import hdbscan\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from HierarchicalGeoClustering.TreeClusters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "050786f1-c002-4a84-ae0f-429e6bd7c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def get_alpha_shape(point_list):\n",
    "    \"\"\"\n",
    "    Returns a polygon representing the hull of the points sample.\n",
    "    \n",
    "    :param list point_list: list list of tuples with samples coordinates.\n",
    "    \n",
    "    :returns shapely.Polygon: concave hull shapely polygon\n",
    "    \"\"\"\n",
    "    uni_po = np.unique(point_list, axis=0)\n",
    "    if len(uni_po) < 3:\n",
    "        raise ValueError('Alpha Shape needs more than 3 points')\n",
    "    if set_colinear(uni_po) == True:\n",
    "        raise ValueError('The set of points can be colinear')\n",
    "\n",
    "    list_of_points = [Point_2(l[0], l[1]) for l in point_list]\n",
    "\n",
    "    a = Alpha_shape_2()\n",
    "\n",
    "    a.make_alpha_shape(list_of_points)\n",
    "    a.set_mode(REGULARIZED)\n",
    "    alpha = a.find_optimal_alpha(1).next()\n",
    "    a.set_alpha(alpha)\n",
    "\n",
    "    edges = []\n",
    "    for it in a.alpha_shape_edges():\n",
    "        edges.append(a.segment(it))\n",
    "\n",
    "    lines = []\n",
    "    for e in edges:\n",
    "        source_p = (e.source().x(), e.source().y())\n",
    "        target_p = (e.target().x(), e.target().y())\n",
    "        lines.append(LineString([source_p, target_p]))\n",
    "\n",
    "    return cascaded_union(list(polygonize(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3a8f35-6709-49ba-9799-1ca8e5f56ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def set_colinear(list_points):\n",
    "    \"\"\"\n",
    "    Check if in the list of points any of triplet of points\n",
    "    is colinear\n",
    "    :param list list_points: List of shapely Points\n",
    "    \n",
    "    :returns bool: True if all are not colinear \n",
    "    \"\"\"\n",
    "    for i in itertools.combinations(list_points, 3):\n",
    "        if collinear(i[0], i[1], i[2]) == False:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee172fb9-626f-4d50-850f-def98bc73bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def collinear(p1, p2, p3):\n",
    "    \"\"\"\n",
    "    Check if the points are colinear \n",
    "    \n",
    "    :param shapely Point p1: point to chek if is colinear\n",
    "    \n",
    "    :param shapely Point p2: point to chek if is colinear\n",
    "    \n",
    "    :param shapely Point p3: point to chek if is colinear\n",
    "    \n",
    "    :return bool: True if are colinear\n",
    "    \"\"\"\n",
    "    return (p1[1]-p2[1]) * (p1[0]-p3[0]) == (p1[1]-p3[1])*(p1[0]-p2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc2bcd9-6a58-4cb9-b971-26a87f7db85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def get_segments(points):\n",
    "    \"\"\" \n",
    "    Get the segments from a delaunay triangulation\n",
    "    \n",
    "    :param points: Point to get Delaunay triangulation and exctract points \n",
    "    \n",
    "    :return edges: \n",
    "    \"\"\"\n",
    "    TIN = Delaunay(points)\n",
    "    # list of coordinates for each edge\n",
    "    edges = []\n",
    "    for tr in TIN.simplices:\n",
    "        for i in range(3):\n",
    "            edge_idx0 = tr[i]\n",
    "            edge_idx1 = tr[(i+1) % 3]\n",
    "            edges.append(LineString((Point(TIN.points[edge_idx0]),\n",
    "                                    Point(TIN.points[edge_idx1]))))\n",
    "\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "469c930c-2913-4b1e-ba9f-d10f41c09caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export \n",
    "def get_polygons_buf(lines):\n",
    "    \"\"\"\n",
    "    Obtain the poligons from the lines\n",
    "    \n",
    "    :param list lines: List of lines\n",
    "    \n",
    "    :returns shapely polygon: the union of the union of \n",
    "    edges (Polygon or multypolygon)\n",
    "    \"\"\"\n",
    "    linework = linemerge(lines)\n",
    "    linework = unary_union(linework)\n",
    "    result, _, _, _ = polygonize_full(linework)\n",
    "    result = unary_union(result)\n",
    "    result = result.buffer(0.0000001)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a69b1da-2f25-4b0a-b749-d6783a12dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export \n",
    "def labels_filtra(point_points, multy_pol):\n",
    "    \"\"\"\n",
    "    Labels the points in the multy_pol if no polygon contains \n",
    "    a point is label as -1\n",
    "    \n",
    "    :param shapely MultyPoint point_points: Points to check \n",
    "    \n",
    "    :param multy_pol\n",
    "    \n",
    "    :returns np.array: Label array with -1 if is not contained \n",
    "    in a polygon\n",
    "    \"\"\"\n",
    "    point_Po = [Point(i) for i in  point_points]\n",
    "    labels_p=[]\n",
    "    if type(multy_pol)==shapely.geometry.MultiPolygon :\n",
    "        for po in point_Po:\n",
    "            if multy_pol.contains(po):\n",
    "                for num_pol, poly in enumerate( multy_pol):\n",
    "                    if poly.contains(po):\n",
    "                        labels_p.append(num_pol)\n",
    "                        break\n",
    "            else:\n",
    "                labels_p.append(-1)\n",
    "    elif type(multy_pol)==shapely.geometry.Polygon :\n",
    "        for po in point_Po:\n",
    "            if multy_pol.contains(po):\n",
    "                labels_p.append(0)\n",
    "            else:\n",
    "                labels_p.append(-1)\n",
    "    else:\n",
    "        raise ValueError('The input is not MultiPolygon or Polygon type')   \n",
    "    \n",
    "    return np.array(labels_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b184b6-62e9-490a-8894-6dbc08cf8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export \n",
    "def levels_from_strings(\n",
    "            string_tag,\n",
    "            level_str='l_',\n",
    "            node_str = 'n_',\n",
    "            **kwargs\n",
    "            ):\n",
    "    \"\"\"\n",
    "    Returns the levels and the node id using the expected strings \n",
    "    that identify the level id and node id\n",
    "    \n",
    "    :param str level_str: string for the level\n",
    "    \n",
    "    :param str node_str: string for the nodes\n",
    "    \n",
    "    \n",
    "    :returns tuple (levels, nodeid): \n",
    "    \"\"\"\n",
    "    \n",
    "    positions = [i.start() for i in re.finditer( level_str, string_tag )]\n",
    "    levels = [string_tag[i+len(level_str)] for i in positions ]\n",
    "    nodeid_positions = [i.start() for i in re.finditer( node_str, string_tag )]    \n",
    "    nodeid = [string_tag[i+len(node_str)] for i in nodeid_positions ]\n",
    "\n",
    "    return levels, nodeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ae62f59-5145-4ff8-88a4-f9f0cafd9dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export \n",
    "def get_tag_level_df_labels(df, levels_int ):\n",
    "    \"\"\"\n",
    "    Get the tag for the cluster\n",
    "    \n",
    "    :param Pandas.DataFrame df:\n",
    "    \n",
    "    :param int levels_int: \n",
    "    \n",
    "    :returns None:\n",
    "    \"\"\"\n",
    "    for i in range(levels_int):\n",
    "        df['level_'+ str(i) +'_cluster']= df['cluster_id'].apply(lambda l:  level_tag(l,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e562702d-253d-401e-93d2-5a72c81e279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export \n",
    "def level_tag(list_tags, level_int  ):\n",
    "    \"\"\"\n",
    "    Tags if the are nois or signal\n",
    "    \"\"\"\n",
    "    if len(list_tags)==0:\n",
    "        return 'noise'\n",
    "    try:\n",
    "        return list_tags[level_int]\n",
    "    except:\n",
    "        return 'noise'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a50db28-fa60-40ad-8085-2ffc0aec7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export \n",
    "def get_dics_labels(tree_or, tree_res, level_get):\n",
    "    \"\"\"\n",
    "    Obtains a list of dictionaries to retag the original tree_tag with their \n",
    "    correspondance in the tree_res on level level_get +1\n",
    "    \n",
    "    :param tree_or:\n",
    "    \n",
    "    :param tree_res:\n",
    "    \n",
    "    :param level_get:\n",
    "    \n",
    "    :param return list:\n",
    "    \"\"\"\n",
    "    dic_list_levels= []\n",
    "    for i in range(level_get):\n",
    "        dic_level_df = get_label_clusters_df(tree_or, tree_res, i)\n",
    "        ## Eliminate the clusters with nan  \n",
    "        dic_level_df.dropna(axis=0, subset=['Sim_cluster'], inplace=True)\n",
    "        \n",
    "        dic_lev  = dic_level_df['Sim_cluster'].to_dict()\n",
    "        dic_list_levels.append({'level_ori':'level_'+str(i)+'_cluster', 'dict': dic_lev})\n",
    "    return dic_list_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e5e8d4-e8f6-4683-bbf6-87795f78aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "\n",
    "def get_label_clusters_df(tree_1, tree_2, level_int):\n",
    "    \"\"\"\n",
    "    Obtains the dataframe with the label \n",
    "    \n",
    "    :param TreeClusters tree_1: \n",
    "    \n",
    "    :param TreeClusters tree_2:\n",
    "    \n",
    "    :param int level_int:\n",
    "    \n",
    "    :reutrns Pandas.DataFrame df_level_clus: \n",
    "    \"\"\"\n",
    "    level_all = tree_1.get_level(level_int)\n",
    "    df_level_clus = pd.DataFrame(level_all, columns=['Clusters'])\n",
    "    df_level_clus['Area'] = df_level_clus['Clusters'].apply(lambda l: l.polygon_cluster.area)\n",
    "    df_level_clus['Name'] = df_level_clus['Clusters'].apply(lambda l: l.name)\n",
    "    \n",
    "    df_level_clus['Sim_cluster'] = df_level_clus['Clusters'].apply(lambda l: get_mini_jaccars(l, tree_2,level_int+1)) ###### Como se hacen las clusterizaciones se debe usar el siguiente nivel\n",
    "    \n",
    "    #print('', df_level_clus['Sim_cluster'].dtype)\n",
    "    df_level_clus= df_level_clus.sort_values(by ='Area', ascending=False)\n",
    "    df_level_clus['Sim_cluster'] = (df_level_clus['Sim_cluster']\n",
    "                                    .where(~df_level_clus.duplicated(subset=['Sim_cluster']), None))\n",
    "    #print(df_level_clus['Sim_cluster'].dtype)\n",
    "    level_2= tree_2.get_level(level_int+1) \n",
    "    df_level_clus['Sim_cluster_name'] =(df_level_clus['Sim_cluster']\n",
    "                                         .astype('int32', errors='ignore')\n",
    "                                         .replace({np.nan: ''})\n",
    "                                         .apply(lambda l:  level_2[int(l)].name if l !=''  else None) )\n",
    "    \n",
    "    return df_level_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a283af-e603-4c1d-8a10-8d4de49911fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export \n",
    "def get_mini_jaccars(cluster, tree_2, level_int):\n",
    "    \"\"\"\n",
    "    Find the most simmilar cluster in the tree_2 at level level_int\n",
    "    returns int the index of the most similar polygon in the level\n",
    "    \"\"\"\n",
    "    tree_2_level= tree_2.get_level(level_int)\n",
    "    Jaccard_i= [jaccard_distance(cluster.polygon_cluster, j.polygon_cluster) for j in tree_2_level]  \n",
    "    \n",
    "    valu_min = Jaccard_i.index( min(Jaccard_i))\n",
    "    return valu_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51d55338-5e1a-4fa7-a5a9-27356b4b201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export \n",
    "def jaccard_distance(p1, p2):\n",
    "    \"\"\"\n",
    "    Computes the Jaccard similarity measuremen between two polygons.\n",
    "    \n",
    "    param: p1 shapely Poligon \n",
    "    param: p2 shapely Poligon \n",
    "    return float Jaccard distance\n",
    "    \"\"\"\n",
    "    intersection_area = p1.intersection(p2).area  \n",
    "    #print(intersection_area)\n",
    "    jacc= 1 - (intersection_area)/(p1.area + p2.area - intersection_area)\n",
    "    return jacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09c33c6f-06e2-4c79-88a0-21c18b264fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export \n",
    "def mod_cid_label(dic_label):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    dic_label={str(k):str(v) for k,v in dic_label.items()} \n",
    "    dic_label['noise'] = 'noise'\n",
    "    return dic_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89579ef9-6e35-4176-a199-361cc0ca8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "def retag_originals(df_fram_or , df_results, tag_original, tag_results, dic_tag_or_res):\n",
    "    \"\"\"\n",
    "    Retags the labels in the df_fram_or using the dictionary dic_tag_or_res to match \n",
    "    the tags with the corresponding tag in the df_result and all the labels that are\n",
    "    not in the dictionary generate a new tag fo them. \n",
    "    \n",
    "    \n",
    "    :param Pandas.DataFrame df_fram_or\n",
    "    \n",
    "    :param Pandas.DataFrame df_results\n",
    "    \n",
    "    :param tag_original\n",
    "    \n",
    "    :param tag_results\n",
    "    \n",
    "    :param Pandas.DataFrame dic_tag_or_res\n",
    "    \n",
    "    \"\"\"\n",
    "    tag_plus=  len(df_results[tag_results].unique()) +100  - len(df_results[tag_results].unique())%100\n",
    "    df_fram_or['re_tag_'+str(df_results.name)+'_'+tag_original] = df_fram_or[tag_original].apply(lambda l: dic_tag_or_res[l] if l in dic_tag_or_res.keys() else   str(int(l) +tag_plus) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fb764f-3cf9-4bd3-8495-c4caed8c30aa",
   "metadata": {},
   "source": [
    "# Clustering \n",
    "\n",
    "In this module all the clustering methods wrap or implemented (Natural cities, DBSCAN, OPTICS, HDBSCAN, and adap_DBSCAN) all with the intention to have the same input and output. \n",
    "\n",
    "A recursive function is implemented to obtain the cluster iterative using the output as the new input. To select the method to use only a string is needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc14de-e93c-436a-a562-cb21130cf99e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fadca08-8cee-4a27-9be1-ae519358afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def clustering(\n",
    "            t_next_level_2,\n",
    "            level=None,\n",
    "            algorithm='dbscan',\n",
    "            **kwargs\n",
    "    ):\n",
    "    \"\"\"Function to get the clusters for single group by\n",
    "    \n",
    "    :param t_next_level_2 Dictionary with the points to compute the\n",
    "            cluster\n",
    "    :param level:  None Level to compute (Default None)\n",
    "    \n",
    "    :param str algorithm : Algorithm type is supported (Default= 'dbscan')\n",
    "    \n",
    "    :param int min_points_cluster:  minimun number of point to consider a cluster(Default 50)\n",
    "    \n",
    "    :param double eps: Epsilon parameter In case is needed\n",
    "    \n",
    "    :param bool return_noise: To return the noise (Default False)\n",
    "    \n",
    "    :param bool verbose: Printing (Dafault False)  \n",
    "    \n",
    "    :returns list t_next_level_n: A list with dictionaries with the points, the parent, and nois \n",
    "    \"\"\"\n",
    "    verbose= kwargs.get('verbose',False)\n",
    "    min_points = kwargs.get( 'min_points_cluster', 50)\n",
    "    ret_noise= kwargs.get('return_noise', False)\n",
    "    eps = kwargs.get('eps',0.8)  # Epsilon value to dbscan\n",
    "    t_next_level_n = []\n",
    "    if level == None:\n",
    "        level = 0\n",
    "\n",
    "    for li_num, cluster_list_D in enumerate(t_next_level_2):\n",
    "        cluster_list = cluster_list_D['points']\n",
    "        cluster_list_pa = cluster_list_D['parent']\n",
    "        if verbose:\n",
    "            print(\"Size cluster list: \", len(cluster_list))\n",
    "            \n",
    "        for c_num, cluster in enumerate(cluster_list):\n",
    "            if verbose:\n",
    "                print(\"Size cluster: \", len(cluster))\n",
    "                print('Algorithm: ', algorithm)\n",
    "\n",
    "            if len(cluster) > 5:\n",
    "                if algorithm == 'dbscan':\n",
    "                    if verbose:\n",
    "                        print(\"Epsilon Value: \", eps)\n",
    "                    tmp = compute_dbscan(cluster,\n",
    "                                 eps_DBSCAN = eps,\n",
    "                                 debugg=verbose,\n",
    "                                  **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "                \n",
    "               \n",
    "                elif algorithm == 'hdbscan':\n",
    "                    tmp = compute_hdbscan(cluster,\n",
    "                                **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "                ##########  \n",
    "                elif algorithm == 'adaptative_DBSCAN':\n",
    "                    #### If the number of cluster is too small \n",
    "                    \n",
    "                    tmp = adaptative_DBSCAN(cluster, **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "\n",
    "                elif algorithm == 'optics':\n",
    "                    tmp = compute_OPTICS(cluster,\n",
    "                                eps_OPTICS = eps,\n",
    "                                **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "                ##########  \n",
    "                elif algorithm == 'natural_cities':\n",
    "                    tmp = compute_Natural_cities(cluster,\n",
    "                                **kwargs)\n",
    "                    if ret_noise:\n",
    "                        noise_points = tmp[1]\n",
    "                        tmp =  tmp[0]\n",
    "                ##########  \n",
    "                else:\n",
    "                    raise ValueError('Algorithm must be dbscan or hdbscan')\n",
    "                    # sys.exit(\"1\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"The number of resulting clusters is : \", len(tmp))\n",
    "                if ret_noise:\n",
    "                    dic_clos = {'points': tmp,\n",
    "                           'parent': cluster_list_pa + '_L_'+str(level) +\n",
    "                            '_l_' + str(li_num) + '_c_'+str(c_num), \n",
    "                            'noise_points':noise_points\n",
    "                    }\n",
    "                else:\n",
    "                    dic_clos = {'points': tmp, 'parent': cluster_list_pa +\n",
    "                            '_L_'+str(level) + '_l_' + str(li_num) + '_c_'+str(c_num)}\n",
    "                \n",
    "                t_next_level_n.append(dic_clos)\n",
    "            else:\n",
    "                if ret_noise:\n",
    "                    dic_clos = {'points': [],\n",
    "                           'parent': cluster_list_pa + '_L_'+str(level) +\n",
    "                            '_l_' + str(li_num) + '_c_'+str(c_num), \n",
    "                            'noise_points':cluster\n",
    "                    }\n",
    "                else:\n",
    "                    dic_clos = {'points': [], 'parent': cluster_list_pa +\n",
    "                            '_L_'+str(level) + '_l_' + str(li_num) + '_c_'+str(c_num)}\n",
    "                t_next_level_n.append(dic_clos)\n",
    "    \n",
    "    return t_next_level_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0b7a559-9d82-4dcc-bd65-0719025bbaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "### Pruebas \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ceb807e-6bd7-47f7-904d-ba6448872f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"clustering\" class=\"doc_header\"><code>clustering</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>clustering</code>(**`t_next_level_2`**, **`level`**=*`None`*, **`algorithm`**=*`'dbscan'`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Function to get the clusters for single group by\n",
       "\n",
       ":param t_next_level_2 Dictionary with the points to compute the\n",
       "        cluster\n",
       ":param level:  None Level to compute (Default None)\n",
       "\n",
       ":param str algorithm : Algorithm type is supported (Default= 'dbscan')\n",
       "\n",
       ":param int min_points_cluster:  minimun number of point to consider a cluster(Default 50)\n",
       "\n",
       ":param double eps: Epsilon parameter In case is needed\n",
       "\n",
       ":param bool return_noise: To return the noise (Default False)\n",
       "\n",
       ":param bool verbose: Printing (Dafault False)  \n",
       "\n",
       ":returns list t_next_level_n: A list with dictionaries with the points, the parent, and nois "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fadc0e6-da7b-4d25-9099-d45d05659d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def recursive_clustering(\n",
    "                this_level,  # Dictionary with Points\n",
    "                to_process,  # levels to process\n",
    "                cluster_tree,  # to store the clusters\n",
    "                level = 0,  # current level\n",
    "                algorithm ='dbscan',  # Algorithm to use\n",
    "                **kwargs\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Performs the recursive clustering.\n",
    "    Calls compute_dbscan for each\n",
    "    list of clusters keepen the structure and then calls itself\n",
    "    until no more clusters satisfy the condition\n",
    "        \n",
    "    :param dict this_level: level is the current level \n",
    "    \n",
    "    :param int to_process: the max level to process\n",
    "    \n",
    "    :param double eps: The epsilon parameter distance to pass to the needed algorithm \n",
    "    \n",
    "    :param list cluster_tree : list of list to insert the levels \n",
    "    \n",
    "    :param bool verbose : To print \n",
    "    \n",
    "    :param double decay: In the use of dbscan the deacy parameter to reduce eps\n",
    "    \n",
    "    :param int min_points_cluster: The min point for each cluster to pass to algorithm\n",
    "    \n",
    "    :param str algorithm:  The string of the algorithm name to use\n",
    "    \"\"\"\n",
    "\n",
    "    verbose= kwargs.get('verbose',False)\n",
    "    min_points = kwargs.get( 'min_points_cluster', 50)\n",
    "    decay = kwargs.get('decay', 0.7)\n",
    "    eps = kwargs.get('eps' ,0.8)  # Epsilon distance to DBSCAN parameter\n",
    "    tmp = None\n",
    "\n",
    "    if level == 0:\n",
    "        kwargs['eps'] = eps\n",
    "    else:\n",
    "        kwargs['eps'] = eps  * decay\n",
    "\n",
    "    cluster_result_polygons = []\n",
    "    if level > to_process:\n",
    "        if verbose:\n",
    "            print('Done clustering')\n",
    "        return\n",
    "    ######## Get the clusters for the current list of points \n",
    "    all_l = clustering(\n",
    "                    this_level,\n",
    "                    level=level,\n",
    "                    algorithm=algorithm,\n",
    "                    \n",
    "                    **kwargs\n",
    "                    )\n",
    "    ##########\n",
    "\n",
    "    cluster_tree.append(all_l)\n",
    "    cluster_n = 0\n",
    "    for i in all_l:\n",
    "        cluster_n += len(i['points'])\n",
    "    if verbose:\n",
    "        print('At level ', level, ' the number of lists are ',\n",
    "              len(all_l), ' with ', cluster_n, 'clusters')\n",
    "    level += 1\n",
    "    if len(all_l) > 0:\n",
    "        return recursive_clustering(all_l, \n",
    "                               to_process=to_process,\n",
    "                               cluster_tree=cluster_tree,\n",
    "                               level= level,\n",
    "                               algorithm=algorithm,\n",
    "                               **kwargs\n",
    "                               )\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('done clustering')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "372eacfa-9a9f-4485-bcfb-95710f57cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "### Pruebas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5e05b61-f672-4d5f-b085-439d0694490d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"recursive_clustering\" class=\"doc_header\"><code>recursive_clustering</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>recursive_clustering</code>(**`this_level`**, **`to_process`**, **`cluster_tree`**, **`level`**=*`0`*, **`algorithm`**=*`'dbscan'`*, **\\*\\*`kwargs`**)\n",
       "\n",
       "Performs the recursive clustering.\n",
       "Calls compute_dbscan for each\n",
       "list of clusters keepen the structure and then calls itself\n",
       "until no more clusters satisfy the condition\n",
       "    \n",
       ":param dict this_level: level is the current level \n",
       "\n",
       ":param int to_process: the max level to process\n",
       "\n",
       ":param double eps: The epsilon parameter distance to pass to the needed algorithm \n",
       "\n",
       ":param list cluster_tree : list of list to insert the levels \n",
       "\n",
       ":param bool verbose : To print \n",
       "\n",
       ":param double decay: In the use of dbscan the deacy parameter to reduce eps\n",
       "\n",
       ":param int min_points_cluster: The min point for each cluster to pass to algorithm\n",
       "\n",
       ":param str algorithm:  The string of the algorithm name to use"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(recursive_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78edffee-7c55-436b-ad3e-0a81d6370aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def recursive_clustering_tree(dic_points_ori, **kwargs):\n",
    "    \"\"\"\n",
    "    Obtaing the recursive tree using a specific algorithm\n",
    "    \"\"\"\n",
    "    levels_clustering= kwargs.get('levels_clustering',4)\n",
    "    algorithm = kwargs.get('algorithm','dbscan')\n",
    "    verbose = kwargs.get('verbose', False)\n",
    "    cluster_tree = []\n",
    "    recursive_clustering([dic_points_ori],  # Dictionary with Points\n",
    "                levels_clustering,  # levels to process\n",
    "                cluster_tree,  # to store the clusters\n",
    "                level=0,  # current level\n",
    "                algorithm = algorithm,  # Algorithm to use\n",
    "                return_noise = True,\n",
    "                verbose=verbose\n",
    "                )\n",
    "    tree_clus= get_tree_from_clustering(cluster_tree)\n",
    "    tree_from_clus= TreeClusters()\n",
    "    tree_from_clus.levels_nodes = tree_clus\n",
    "    tree_from_clus.root= tree_from_clus.levels_nodes[0][0]   \n",
    "    return tree_from_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e96a3-7c17-46cf-9d50-64e1a3aa1f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "215b9b2a-9ced-4043-87bd-950235e99ed6",
   "metadata": {},
   "source": [
    "## Clustering Algorithms\n",
    "A wapper functions to obtain the clusterizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3646c9cf-05c5-4d02-a3db-030b3834ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_dbscan(cluster,  **kwargs):\n",
    "    \n",
    "    \"\"\" \n",
    "    Sklearn DBSCAN wrapper.\n",
    "    \n",
    "    :param cluster: a (N,2) numpy array containing the obsevations\n",
    "\n",
    "    :returns list with numpy arrays for all the clusters obtained\n",
    "    \"\"\"\n",
    "    eps = kwargs.get( 'eps_DBSCAN',.04)\n",
    "    debugg= kwargs.get( 'debugg',False)\n",
    "    min_samples= kwargs.get( 'min_samples',50)\n",
    "    ret_noise = kwargs.get('return_noise', False)\n",
    "    # Standarize sample\n",
    "    scaler = StandardScaler()\n",
    "    cluster = scaler.fit_transform(cluster)\n",
    "    if debugg:\n",
    "        print('epsilon distance to DBSCAN: ', eps)\n",
    "        print(\"min_samples to DBScan: \", min_samples )\n",
    "        print(\"Number of points to fit the DBScan: \",cluster.shape[0])\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples).fit(cluster)  # Check if can be run with n_jobs = -1\n",
    "    \n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    labels = db.labels_\n",
    "    l_unique_labels = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    unique_labels = set(labels) \n",
    "    cluster = scaler.inverse_transform(cluster)\n",
    "    clusters = []\n",
    "    if debugg:\n",
    "        print('Number of clusters:' ,l_unique_labels)\n",
    "    \n",
    "    for l in unique_labels:\n",
    "        if l != -1:\n",
    "            class_member_mask = (labels == l)\n",
    "            clusters.append(cluster[class_member_mask])\n",
    "        elif l == -1 and debugg == True:\n",
    "            class_member_mask = (labels == l)\n",
    "            print(\"Muestras consideradas ruido: \",  sum(class_member_mask))\n",
    "    \n",
    "    if ret_noise == True:\n",
    "        class_member_mask = (labels == -1)\n",
    "        return clusters, cluster[class_member_mask]\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa88e40b-6f7a-480a-a346-25146d817c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "### Pruebas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b71a439b-dcb8-45da-9e27-aa9374dca393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"compute_dbscan\" class=\"doc_header\"><code>compute_dbscan</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>compute_dbscan</code>(**`cluster`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Sklearn DBSCAN wrapper.\n",
       "\n",
       ":param cluster: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":returns list with numpy arrays for all the clusters obtained"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(compute_dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1e2e2cc-3ead-46df-913d-81b233979560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def adaptative_DBSCAN(points2_clusters ,\n",
    "                **kwargs):\n",
    "    \"\"\"\n",
    "    The function use the knee and average to obtain a good value for epsilon and use \n",
    "    DBSCAN to obtain the clusters\n",
    "    \n",
    "    :param list Points points2_clusters: Point to clusterize  \n",
    "    \n",
    "    :param int max_k: = (Default = len(points2_clusters)*.1)\n",
    "    \n",
    "    :param int  min_k: (Default =50)\n",
    "    \n",
    "    :param int step_k: (Default = 50)\n",
    "    \n",
    "    :param int leaf_size: (Default = 50)\n",
    "    \n",
    "    :param bool scale_points: (Default = True)\n",
    "    \n",
    "    :param bool debugg: (Default = False)\n",
    "    \n",
    "    :param bool ret_noise:  (Default = True)\n",
    "    \n",
    "    :returns list : list of cluster. If ret_noise = True return tuple list of cluter and noise \n",
    "    \"\"\"\n",
    "    max_k = kwargs.get('max_k', int(len(points2_clusters)*.1))\n",
    "    min_k = kwargs.get('min_k', 50)\n",
    "    step_k = kwargs.get('step_k', 50)\n",
    "    leaf_size =  kwargs.get('leaf_size',50)\n",
    "    scale_points= kwargs.get('scale_points',True)\n",
    "    debugg = kwargs.get('verbose',False)\n",
    "    ret_noise = kwargs.get('return_noise', True)\n",
    "    ###### Se tienen que hacer algunos cambios para cuando\n",
    "    #  los clusters son menores a los minimos establecidos previemente\n",
    "    \n",
    "    ##### Establecer los minimos posibles \n",
    "    if max_k > len(points2_clusters):\n",
    "        raise ValueError('The max_k value is too large for the number of points')\n",
    "    \n",
    "    if min_k >  len(points2_clusters):\n",
    "        print('The min_k value is too large for the number of points returns empty clusters')\n",
    "        if ret_noise == True:\n",
    "            return [] , points2_clusters\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    if step_k > len(points2_clusters):\n",
    "        raise ValueError('The step_k value is too large for the number of points')\n",
    "\n",
    "    \n",
    "    if min_k == max_k:\n",
    "        print('min_k reset to obtain at least 1 value')\n",
    "        min_k = max_k-1\n",
    "\n",
    "    if scale_points ==True:\n",
    "        scaler = StandardScaler()\n",
    "        points_arr = scaler.fit_transform(points2_clusters)\n",
    "    else:\n",
    "        points_arr = points2_clusters\n",
    "    \n",
    "    kdt=  cKDTree(points_arr, leafsize=leaf_size)\n",
    "    lits_appe_all_aver=[]\n",
    "    for j in range( min_k, max_k, step_k ):\n",
    "        dist_va, ind = kdt.query(points_arr, k=j, n_jobs =-1) \n",
    "        non_zero =  dist_va[:, 1:]\n",
    "        non_zero = np.ndarray.flatten(non_zero)\n",
    "        non_zero = np.sort(non_zero)\n",
    "        lis_aver_k=[]\n",
    "        for i in range(int(non_zero.shape[0]/(j-1)) -1):\n",
    "            lis_aver_k.append(np.average(non_zero[i*(j-1):(i+1)*(j-1)]))\n",
    "\n",
    "        average_arr= np.array(lis_aver_k)\n",
    "        kneedle_1_average = kneed.KneeLocator(\n",
    "                range(average_arr.shape[0]),\n",
    "                average_arr,\n",
    "                curve=\"convex\",## This should be the case since the values are sorted \n",
    "                direction=\"increasing\", ## This should be the case since the values are sorted incresing\n",
    "                online=True, ### To find the correct knee the false returns the first find \n",
    "        )\n",
    "        epsilon= kneedle_1_average.knee_y\n",
    "        min_point = kneedle_1_average.knee\n",
    "        #### We take the average never the less\n",
    "        \n",
    "        lits_appe_all_aver.append({ 'k':j,\n",
    "                    'Epsilon':epsilon,\n",
    "                    'value':min_point})\n",
    "    \n",
    "    #### Check if the list is empty\n",
    "    if len(lits_appe_all_aver) ==0:\n",
    "        if debugg:\n",
    "            print('DBSCAN')\n",
    "            print('Using 0.6 as epsilon and 20 as Minpoints')\n",
    "        db_scan= DBSCAN(eps=0.6, min_samples=20).fit(points_arr)\n",
    "    else:\n",
    "        df_all_average= pd.DataFrame(lits_appe_all_aver)\n",
    "        max_epsi_all_average= df_all_average['Epsilon'].max()\n",
    "        if debugg:\n",
    "            print('Valor de epsion  : ', max_epsi_all_average)\n",
    "        db_scan= DBSCAN(eps=max_epsi_all_average, min_samples=min_k).fit(points_arr)\n",
    "    \n",
    "    ####Get the clusters\n",
    "    core_samples_mask = np.zeros_like(db_scan.labels_, dtype=bool)\n",
    "    core_samples_mask[db_scan.core_sample_indices_] = True\n",
    "    labels = db_scan.labels_\n",
    "    unique_labels = set(labels)\n",
    "    if scale_points ==True:\n",
    "        points_ret = scaler.inverse_transform(points_arr)\n",
    "    else:\n",
    "        points_ret = points_arr\n",
    "    clusters = []\n",
    "    for l in unique_labels:\n",
    "        if l != -1:\n",
    "            class_member_mask = (labels == l)\n",
    "            clusters.append(points_ret[class_member_mask])\n",
    "        elif l == -1 and debugg == True:\n",
    "            class_member_mask = (labels == l)\n",
    "            print(\"Muestras consideradas ruido: \",  sum(class_member_mask))\n",
    "\n",
    "    if ret_noise == True:\n",
    "        class_member_mask = (labels == -1)\n",
    "        return clusters, points_ret[class_member_mask]\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "100a16d3-40bc-4587-ba39-35a2305fceff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"adaptative_DBSCAN\" class=\"doc_header\"><code>adaptative_DBSCAN</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>adaptative_DBSCAN</code>(**`points2_clusters`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "The function use the knee and average to obtain a good value for epsilon and use \n",
       "DBSCAN to obtain the clusters\n",
       "\n",
       ":param list Points points2_clusters: Point to clusterize  \n",
       "\n",
       ":param int max_k: = (Default = len(points2_clusters)*.1)\n",
       "\n",
       ":param int  min_k: (Default =50)\n",
       "\n",
       ":param int step_k: (Default = 50)\n",
       "\n",
       ":param int leaf_size: (Default = 50)\n",
       "\n",
       ":param bool scale_points: (Default = True)\n",
       "\n",
       ":param bool debugg: (Default = False)\n",
       "\n",
       ":param bool ret_noise:  (Default = True)\n",
       "\n",
       ":returns list : list of cluster. If ret_noise = True return tuple list of cluter and noise "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(adaptative_DBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "408c167d-92a4-4220-9c06-14e536b6dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_hdbscan(points2_clusters,  **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    HDBSCAN wrapper.\n",
    "    \n",
    "    :param np.array cluster: a (N,2) numpy array containing the obsevations\n",
    "    \n",
    "    :returns:  list with numpy arrays for all the clusters obtained\n",
    "    \"\"\"\n",
    "    \n",
    "    scale_points= kwargs.get('scale_points',True)\n",
    "    debugg = kwargs.get('verbose',False)\n",
    "    ret_noise = kwargs.get('return_noise', True)\n",
    "    min_cluster = kwargs.get('min_cluster', 20)\n",
    "    if scale_points ==True:\n",
    "        scaler = StandardScaler()\n",
    "        points_arr = scaler.fit_transform(points2_clusters)\n",
    "    else:\n",
    "        points_arr = points2_clusters\n",
    "\n",
    "    db = hdbscan.HDBSCAN( ).fit(points_arr)\n",
    "    core_samples_mask = np.full_like(db.labels_, True, dtype=bool)\n",
    "    labels = db.labels_\n",
    "    l_unique_labels = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    unique_labels = set(labels)\n",
    "    if debugg:\n",
    "        print('total number of clusters: ', len(unique_labels)) \n",
    "    if scale_points ==True:\n",
    "        points_ret = scaler.inverse_transform(points_arr)\n",
    "    else:\n",
    "        points_ret = points_arr\n",
    "    clusters = []\n",
    "\n",
    "    for l in unique_labels:\n",
    "        if l != -1:\n",
    "            class_member_mask = (labels == l)\n",
    "            clusters.append(points_ret[class_member_mask])\n",
    "        elif l == -1 and debugg == True:\n",
    "            class_member_mask = (labels == l)\n",
    "            print(\"Muestras consideradas ruido: \",  sum(class_member_mask))\n",
    "\n",
    "    if ret_noise == True:\n",
    "        class_member_mask = (labels == -1)\n",
    "        return clusters, points_ret[class_member_mask]\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8517503-4f13-4b67-9970-3f49c058d540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"compute_hdbscan\" class=\"doc_header\"><code>compute_hdbscan</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>compute_hdbscan</code>(**`points2_clusters`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "HDBSCAN wrapper.\n",
       "\n",
       ":param np.array cluster: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":returns:  list with numpy arrays for all the clusters obtained"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(compute_hdbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1799aca1-c7a2-48aa-8e1e-bc0d037bcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compute_OPTICS(points2_clusters,  **kwargs):\n",
    "    \n",
    "    \"\"\" OPTICS wrapper.\n",
    "    :param np.array cluster: a (N,2) numpy array containing the obsevations\n",
    "    :returns:  list with numpy arrays for all the clusters obtained\n",
    "    \"\"\"\n",
    "\n",
    "    scale_points= kwargs.get('scale_points',True)\n",
    "    debugg = kwargs.get('verbose',False)\n",
    "    ret_noise = kwargs.get('return_noise', True)\n",
    "    min_samples= kwargs.get( 'min_samples',5)\n",
    "    eps_optics = kwargs.get('eps_optics', None)\n",
    "    n_jobs = kwargs.get('num_jobs',None)\n",
    "    xi= kwargs.get('xi',None)\n",
    "    algorithm_optics= kwargs.get('algorithm_optics','kd_tree')\n",
    "\n",
    "    if scale_points ==True:\n",
    "        scaler = StandardScaler()\n",
    "        points_arr = scaler.fit_transform(points2_clusters)\n",
    "    else:\n",
    "        points_arr = points2_clusters\n",
    "\n",
    "\n",
    "    db = OPTICS(min_samples = min_samples,eps= eps_optics, n_jobs= n_jobs).fit(points2_clusters)\n",
    "    core_samples_mask = np.full_like(db.labels_, True, dtype=bool)\n",
    "    labels = db.labels_\n",
    "    l_unique_labels = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    unique_labels = set(labels)\n",
    "    if debugg:\n",
    "        print('total number of clusters: ', len(unique_labels)) \n",
    "    if scale_points ==True:\n",
    "        points_ret = scaler.inverse_transform(points_arr)\n",
    "    else:\n",
    "        points_ret = points_arr\n",
    "    clusters = []\n",
    "\n",
    "    for l in unique_labels:\n",
    "        if l != -1:\n",
    "            class_member_mask = (labels == l)\n",
    "            clusters.append(points_ret[class_member_mask])\n",
    "        elif l == -1 and debugg == True:\n",
    "            class_member_mask = (labels == l)\n",
    "            print(\"Muestras consideradas ruido: \",  sum(class_member_mask))\n",
    "\n",
    "    if ret_noise == True:\n",
    "        class_member_mask = (labels == -1)\n",
    "        return clusters, points_ret[class_member_mask]\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b680f5ea-a2fd-4321-a773-5ac8395c3bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"compute_OPTICS\" class=\"doc_header\"><code>compute_OPTICS</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>compute_OPTICS</code>(**`points2_clusters`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "OPTICS wrapper.\n",
       ":param np.array cluster: a (N,2) numpy array containing the obsevations\n",
       ":returns:  list with numpy arrays for all the clusters obtained"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(compute_OPTICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8e9777d-c78a-446d-ac64-971ac2acdea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def compute_Natural_cities(points2_clusters,  **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute Natural cities clustering\n",
    "    \n",
    "    :param np.array points2_clusters: a (N,2) numpy array containing the obsevations\n",
    "    \n",
    "    :returns: list with numpy arrays for all the clusters obtained\n",
    "    \"\"\"\n",
    "    ### The function is in acordance with the all the previus functions\n",
    "    scale_points= kwargs.get('scale_points',True)\n",
    "    debugg = kwargs.get('verbose',False)\n",
    "    ret_noise = kwargs.get('return_noise', True)\n",
    "\n",
    "    if scale_points ==True:\n",
    "        scaler = StandardScaler()\n",
    "        points_arr = scaler.fit_transform(points2_clusters)\n",
    "    else:\n",
    "        points_arr = points2_clusters\n",
    "\n",
    "    edges= get_segments(points_arr)\n",
    "    lenght_av  =  np.average(np.array([i.length for i in edges ]))\n",
    "    edges = [i for i in edges  if i.length < lenght_av]\n",
    "    polygons_natural_cities=  get_polygons_buf(edges)\n",
    "    if debugg:\n",
    "        if type(polygons_natural_cities)==shapely.geometry.MultiPolygon:\n",
    "            print('Resulting number of polygons: ', len(polygons_natural_cities))\n",
    "\n",
    "        elif type(polygons_natural_cities)==shapely.geometry.Polygon:\n",
    "            print('Only 1 polygon: ')\n",
    "        else:\n",
    "            print('The result is not a Polygon or Multipolygon')\n",
    "    labels_points = labels_filtra(points_arr, polygons_natural_cities)\n",
    "    core_samples_mask = np.full_like(labels_points, True, dtype=bool)\n",
    "    l_unique_labels = len(set(labels_points)) - (1 if -1 in labels_points else 0)\n",
    "    unique_labels = set(labels_points)\n",
    "    \n",
    "    if debugg:\n",
    "        print('total number of clusters: ', len(unique_labels)) \n",
    "    #### recover\n",
    "    if scale_points ==True:\n",
    "        points_ret = scaler.inverse_transform(points_arr)\n",
    "    else:\n",
    "        points_ret = points_arr\n",
    "\n",
    "    \n",
    "    clusters = []\n",
    "    for l in unique_labels:\n",
    "        if l != -1:\n",
    "            class_member_mask = (labels_points == l)\n",
    "            clusters.append(points_ret[class_member_mask])\n",
    "        elif l == -1 and debugg == True:\n",
    "            class_member_mask = (labels_points == l)\n",
    "            print(\"Point conscider noise: \",  sum(class_member_mask))\n",
    "\n",
    "    if ret_noise == True:\n",
    "        class_member_mask = (labels_points == -1)\n",
    "        return clusters, points_ret[class_member_mask]\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c071ebd-4dba-414d-bf72-c1dd4f84566e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"compute_Natural_cities\" class=\"doc_header\"><code>compute_Natural_cities</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>compute_Natural_cities</code>(**`points2_clusters`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "Compute Natural cities clustering\n",
       "\n",
       ":param np.array points2_clusters: a (N,2) numpy array containing the obsevations\n",
       "\n",
       ":returns: list with numpy arrays for all the clusters obtained"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(compute_Natural_cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b244b2-2472-481e-97ca-ef84fde2b59e",
   "metadata": {},
   "source": [
    "## Similarity form metric\n",
    "\n",
    "The function use the *Jaccar* index to and the points in the clusterization to obtain a metric tha is use to compare the clusters on level basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d1315c8-1cfb-41aa-ab33-f4db9bbab421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def SSM(list_poly_c_1,list_poly_c_2 ,**kwargs):\n",
    "    \"\"\"\n",
    "    The function calculates the Similarity Form Measurment (SMF)\n",
    "    between two clusterizations \n",
    "    \n",
    "    :param: list of nodes with points and polygons \n",
    "    \n",
    "    :param: list of nodes with points and polygons \n",
    "    \n",
    "    :param bool verbose: To print to debugg\n",
    "    \n",
    "    :returns double: The similarity mesuarment\n",
    "    \"\"\"\n",
    "    verbose= kwargs.get('verbose', False)\n",
    "    ##### Get intersection \n",
    "    list_de=[]\n",
    "    for i in list_poly_c_1:\n",
    "        list_de.append([ i.polygon_cluster.intersection(  j.polygon_cluster ) for  j in list_poly_c_2])\n",
    "    \n",
    "    list_de_bool = []\n",
    "    for i in list_de:\n",
    "        list_de_bool.append([not j.is_empty for j in i])\n",
    "    \n",
    "    list_de_index = []\n",
    "    #print(list_de_bool)\n",
    "    for i in list_de_bool:\n",
    "        if any(i):\n",
    "            list_de_index.append(i.index(True))\n",
    "        else:\n",
    "            list_de_index.append(None)\n",
    "    jacc_sim_po = []\n",
    "    for num, node in enumerate(list_poly_c_1):\n",
    "        ### ver eque pasa cuando se tienen 2 \n",
    "        if list_de_index[num] is not None:\n",
    "            node_get = list_poly_c_2[list_de_index[num]]\n",
    "            poli_int = list_de[num][list_de_index[num]]\n",
    "            \n",
    "            ####Puntos en la interseccion\n",
    "            #print(node)\n",
    "            points_all = node.get_point_decendent()\n",
    "            res_bool =[ poli_int.contains(p) for p in points_all] ### Como no necesito los puntos basta con esto\n",
    "            card = sum(res_bool)\n",
    "            ###Obtenemos jaccard \n",
    "            sim_jacc = (poli_int.area)/(node.polygon_cluster.area + node_get.polygon_cluster.area - poli_int.area)\n",
    "            if verbose:\n",
    "                print(\"jaccard: \" ,sim_jacc)\n",
    "                print(\"cardinal: \" ,card )\n",
    "            jacc_sim_po.append(sim_jacc* card)#####Cuando hay interseccion \n",
    "        else:\n",
    "            jacc_sim_po.append(0) #### Cuando no hay\n",
    "    \n",
    "    arr_bool = np.array(list_de_bool)\n",
    "\n",
    "    Q_not= []\n",
    "    for col in range(arr_bool.shape[1]):\n",
    "        cols_sel= arr_bool[:,col].any()\n",
    "        if cols_sel ==False:\n",
    "            Q_not.append(col)\n",
    "    #print(Q_not)\n",
    "    len_Q_not=[]\n",
    "    if Q_not:\n",
    "         len_Q_not =[len(list_poly_c_2[i].get_point_decendent())  for i in Q_not] \n",
    "\n",
    "    P_sum = sum([len(node.get_point_decendent())  for node in list_poly_c_1])\n",
    "    deno =P_sum + sum(len_Q_not)\n",
    "    return sum(jacc_sim_po)/deno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a050fa-31b3-4d9f-bf4e-bde5a859353e",
   "metadata": {},
   "source": [
    "## Clustering an hieralchical structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "641868a1-0290-473f-ad20-688ab8635ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_tree_from_clustering(cluster_tree_clusters):\n",
    "     \"\"\" Returns the tree from the iterative clustering, the cluster_tree_cluster\n",
    "     \n",
    "     :param cluster_tree_clusters is a list of list with a dictionary that \n",
    "            should contain the point of the next level clusters, the name of the parent\n",
    "            cluster of such clusters (name of the current node), and the point that \n",
    "            are consider noise. \n",
    "    \n",
    "     :return A list of list that contains the nodes for each level of the tree. \n",
    "     \"\"\" \n",
    "     ##### La estructura de arbol \n",
    "     all_level_clusters =[]\n",
    "     previus_level=[]\n",
    "     list_len = len(cluster_tree_clusters)-1\n",
    "     for level_num_clus, level_cluster in enumerate(cluster_tree_clusters):\n",
    "          level_nodes=[]\n",
    "          for cluster_te in level_cluster:\n",
    "               node_l = NodeCluster(name= cluster_te['parent'])\n",
    "               to_concat= [point_arr for point_arr in cluster_te['points']]\n",
    "               if len(to_concat)>0: \n",
    "                    points_poly = np.concatenate(to_concat)\n",
    "               else:\n",
    "                    points_poly = np.array([], dtype= np.float64).reshape(0,2)\n",
    "               \n",
    "               points_poly = np.concatenate(\n",
    "                                   (points_poly,cluster_te['noise_points']),\n",
    "                                   axis=0\n",
    "                              )\n",
    "               \n",
    "               node_l.polygon_cluster =  get_alpha_shape(points_poly)\n",
    "               ##### Es necesario que si es el último nivel todos los puntoas sean\n",
    "               ## considerados como ruido pues aunque se haya hecho la clusterizacion\n",
    "               #  ya no se bajo al siguiente nivel\n",
    "               ## \n",
    "               if level_num_clus==list_len:\n",
    "                    node_l.point_cluster_noise = shapely.geometry.MultiPoint(points_poly) \n",
    "               else:\n",
    "                    node_l.point_cluster_noise = shapely.geometry.MultiPoint(cluster_te['noise_points']) \n",
    "               pos=node_l.name.rfind('_L')\n",
    "               if node_l.name[:pos]=='':\n",
    "                    node_l.parent = None\n",
    "               else:\n",
    "                    lis_pa=[item  for item in previus_level if item.name == node_l.name[:pos]]\n",
    "                    node_l.parent= lis_pa[0]\n",
    "               level_nodes.append(node_l)\n",
    "          all_level_clusters.append(level_nodes)\n",
    "          previus_level = level_nodes\n",
    "     return  all_level_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed8da47d-30f6-4526-a36c-310b2d09f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "\n",
    "def generate_tree_clusterize_form(**kwargs ):\n",
    "    \"\"\"\n",
    "    Generates all the experiment all the experiment creates the data and clusterize using the algorithm available\n",
    "    \n",
    "    :param levels_tree: Levels for the tree\n",
    "    \n",
    "    :param int per_cluster: Points per clusters\n",
    "    \n",
    "    :param levels_cluster:  Levels to clusterize\n",
    "    \n",
    "    :param bool verbose:  To print some outputs \n",
    "    \n",
    "    :returns: a dictionary with all the  data frames and a dictionary \n",
    "    with the similarity measurment created\n",
    "     \"\"\"\n",
    "    \n",
    "    levels_tree= kwargs.get('tree_level', 4)\n",
    "    per_cluster = kwargs.get('num_per_cluster', 200)\n",
    "    levels_cluster = kwargs.get('levels_cluster', 4)\n",
    "    verbose = kwargs.get('verbose', False)\n",
    "    \n",
    "    if verbose:\n",
    "        print('generating tree')\n",
    "    \n",
    "    \n",
    "    random.seed(int(time.time()))\n",
    "    random_seed = random.randint(0,1500)\n",
    "    print('Random to use: ',random_seed )\n",
    "    print('With',levels_tree , ' levels' )\n",
    "    tree_original= TreeClusters(levels_tree, random_seed= random_seed)\n",
    "    tree_original.populate_tree(number_per_cluster=per_cluster, avoid_intersec= True)\n",
    "    tree_original_points= tree_original.get_points_tree()\n",
    "    X_2=np.array([[p.x,p.y] for p in tree_original_points])\n",
    "    dic_points_ori={'points':[X_2], 'parent':''}\n",
    "    if verbose:\n",
    "        print('tree with: ', X_2.shape )\n",
    "    \n",
    "    while X_2.shape[0] < 2000:\n",
    "        \n",
    "        print('tree with too few elements to clusterize creating new tree')\n",
    "        random.seed(int(time.time()))\n",
    "        random_seed = random.randint(0,1500)\n",
    "        print('Random to use: ',random_seed )\n",
    "        tree_original= TreeClusters(levels_tree, random_seed= random_seed)\n",
    "        tree_original.populate_tree(number_per_cluster=per_cluster, avoid_intersec= True)\n",
    "        tree_original_points= tree_original.get_points_tree()\n",
    "        X_2=np.array([[p.x,p.y] for p in tree_original_points])\n",
    "        dic_points_ori={'points':[X_2], 'parent':''}\n",
    "        \n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print('tree generated')\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        print('clusterize and creating the trees')\n",
    "    \n",
    "    tree_Natural_c = recursive_clustering_tree(dic_points_ori,\n",
    "                                               levels_clustering = levels_cluster,\n",
    "                                              algorithm = 'natural_cities'\n",
    "                                              )\n",
    "    tree_DBSCAN = recursive_clustering_tree(dic_points_ori,\n",
    "                                               levels_clustering = levels_cluster,\n",
    "                                              algorithm = 'dbscan')\n",
    "    tree_HDBSCAN = recursive_clustering_tree(dic_points_ori,\n",
    "                                               levels_clustering = levels_cluster,\n",
    "                                              algorithm = 'hdbscan')\n",
    "    tree_OPTICS= recursive_clustering_tree(dic_points_ori,\n",
    "                                               levels_clustering = levels_cluster,\n",
    "                                              algorithm = 'optics')\n",
    "    tree_knee = recursive_clustering_tree(dic_points_ori,\n",
    "                                               levels_clustering = levels_cluster,\n",
    "                                              algorithm = 'adaptative_DBSCAN')\n",
    "    if verbose:\n",
    "        print('DONE clusterize and creating the trees')\n",
    "    ######  get the points dataframe for each tree \n",
    "    data_fram_or = tree_original.get_dataframe_recursive_node_label(func_level_nodes = levels_from_strings)\n",
    "    df_Natural = tree_Natural_c.get_dataframe_recursive_node_label()\n",
    "    df_DBSCAN = tree_DBSCAN.get_dataframe_recursive_node_label()\n",
    "    df_HDBSCAN = tree_HDBSCAN.get_dataframe_recursive_node_label()\n",
    "    df_OPTICS = tree_OPTICS.get_dataframe_recursive_node_label()\n",
    "    df_knee = tree_knee.get_dataframe_recursive_node_label()\n",
    "    \n",
    "    df_Natural.name='Natural_C'\n",
    "    df_DBSCAN.name= 'DBSCAN'\n",
    "    df_HDBSCAN.name= 'HDBSCAN'\n",
    "    df_OPTICS.name= 'OPTICS'\n",
    "    df_knee.name = 'knee'\n",
    "    \n",
    "    if verbose:\n",
    "        print('Original size',data_fram_or.shape )\n",
    "        print('Natural size',df_Natural.shape)\n",
    "        print('DBSCAN size',df_DBSCAN.shape)\n",
    "        print('HDBSCAN size',df_HDBSCAN.shape)\n",
    "        print('OPTICS size',df_OPTICS.shape)\n",
    "        print('adaptative_DBSCAN size',df_knee.shape)\n",
    "    \n",
    "    ######For each dataframe  \n",
    "    if verbose:\n",
    "        print('get dataframe Original')\n",
    "    get_tag_level_df_labels(data_fram_or, levels_cluster)\n",
    "    ###Natural Cities\n",
    "    if verbose:\n",
    "        print('get dataframe Natural cities')\n",
    "        \n",
    "    dic_final_levels_Natural_c = get_dics_labels(tree_original, tree_Natural_c, levels_cluster)\n",
    "    dic_label_final_levels_Natural=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_Natural_c]\n",
    "    get_tag_level_df_labels(df_Natural, levels_cluster)\n",
    "    for dic in dic_label_final_levels_Natural[1:]: ## En el nivel 0 no tiene sentido\n",
    "        tag_ori = dic['level_ori']\n",
    "        dic_lev = dic['dict']\n",
    "        retag_originals(data_fram_or,\n",
    "                        df_Natural,\n",
    "                        tag_ori,\n",
    "                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "                        dic_lev)\n",
    "    \n",
    "    \n",
    "    ### DBSCAN\n",
    "    if verbose:\n",
    "        print('get dataframe DBSCAN')\n",
    "        \n",
    "    dic_final_levels_DBSCAN = get_dics_labels(tree_original, tree_DBSCAN, levels_cluster)\n",
    "    dic_label_final_levels_DBSCAN=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_DBSCAN]\n",
    "    get_tag_level_df_labels(df_DBSCAN, levels_cluster)\n",
    "    for dic in dic_label_final_levels_DBSCAN[1:]: ## En el nivel 0 no tiene sentido\n",
    "        tag_ori = dic['level_ori']\n",
    "        dic_lev = dic['dict']\n",
    "        retag_originals(data_fram_or,\n",
    "                        df_DBSCAN,\n",
    "                        tag_ori,\n",
    "                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "                        dic_lev)\n",
    "    \n",
    "    \n",
    "    ##HDBSCAN\n",
    "    if verbose:\n",
    "        print('get dataframe HDBSCAN')\n",
    "    dic_final_levels_HDBSCAN = get_dics_labels(tree_original, tree_HDBSCAN, levels_cluster)\n",
    "    dic_label_final_levels_HDBSCAN=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_HDBSCAN]\n",
    "    get_tag_level_df_labels(df_HDBSCAN, levels_cluster)\n",
    "    for dic in dic_label_final_levels_HDBSCAN[1:]: ## En el nivel 0 no tiene sentido\n",
    "        tag_ori = dic['level_ori']\n",
    "        dic_lev = dic['dict']\n",
    "        retag_originals(data_fram_or,\n",
    "                        df_HDBSCAN,\n",
    "                        tag_ori,\n",
    "                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "                        dic_lev)\n",
    "    #### OPTICS\n",
    "    if verbose:\n",
    "        print('get dataframe OPTICS')\n",
    "        \n",
    "    dic_final_levels_OPTICS = get_dics_labels(tree_original, tree_OPTICS, levels_cluster)\n",
    "    dic_label_final_levels_OPTICS=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_OPTICS]\n",
    "    get_tag_level_df_labels(df_OPTICS, levels_cluster)\n",
    "    for dic in dic_label_final_levels_OPTICS[1:]: ## En el nivel 0 no tiene sentido\n",
    "        tag_ori = dic['level_ori']\n",
    "        dic_lev = dic['dict']\n",
    "        retag_originals(data_fram_or,\n",
    "                        df_OPTICS,\n",
    "                        tag_ori,\n",
    "                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "                        dic_lev)\n",
    "    ##### Knee\n",
    "    if verbose:\n",
    "        print('get adaptative DBSCAN')\n",
    "        \n",
    "    dic_final_levels_knee = get_dics_labels(tree_original, tree_knee, levels_cluster)\n",
    "    dic_label_final_levels_knee=[ {'level_ori':dic['level_ori'], 'dict':mod_cid_label(dic['dict']) } for dic in  dic_final_levels_knee]\n",
    "    get_tag_level_df_labels(df_knee, levels_cluster)\n",
    "    for dic in dic_label_final_levels_knee[1:]: ## En el nivel 0 no tiene sentido\n",
    "        tag_ori = dic['level_ori']\n",
    "        dic_lev = dic['dict']\n",
    "        retag_originals(data_fram_or,\n",
    "                        df_knee,\n",
    "                        tag_ori,\n",
    "                        tag_ori,#### Como se hizo con la misma funcion tienen las misma etiqueta \n",
    "                        dic_lev)\n",
    "    ##############################################Get only signal  noise \n",
    "    data_fram_or_sig_noise = tree_original.get_tag_noise_signal_tree()\n",
    "    df_Natural_sig_noise = tree_Natural_c.get_tag_noise_signal_tree()\n",
    "    df_DBSCAN_sig_noise = tree_DBSCAN.get_tag_noise_signal_tree()\n",
    "    df_HDBSCAN_sig_noise = tree_HDBSCAN.get_tag_noise_signal_tree()\n",
    "    df_OPTICS_sig_noise = tree_OPTICS.get_tag_noise_signal_tree()\n",
    "    df_knee_sig_noise = tree_knee.get_tag_noise_signal_tree()\n",
    "    \n",
    "    df_Natural_sig_noise.name='I_Natural_C'\n",
    "    df_DBSCAN_sig_noise.name= 'I_DBSCAN'\n",
    "    df_HDBSCAN_sig_noise.name= 'I_HDBSCAN'\n",
    "    df_OPTICS_sig_noise.name= 'I_OPTICS'\n",
    "    df_knee_sig_noise.name = 'I_Ada_DBSCAN'\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ######Evaluate form metric\n",
    "    if verbose:\n",
    "        print('Niveles: ' ,len(tree_original.levels_nodes))\n",
    "        print('Nodos en el ultimo nivel: ' ,len(tree_original.levels_nodes[-1]))\n",
    "        #print('tag_all : ' , data_fram_or['Final_tag'].unique())\n",
    "    levels_r = range(0, levels_cluster)###\n",
    "    resultado_form_metric = []\n",
    "    for l in levels_r:\n",
    "        \n",
    "        d = { 'Level': l,\n",
    "            'DBSCAN': SSM(tree_original.levels_nodes[l],\n",
    "                                            tree_DBSCAN.levels_nodes[l]),\n",
    "             'HDBSCAN': SSM(tree_original.levels_nodes[l],\n",
    "                                             tree_HDBSCAN.levels_nodes[l]),\n",
    "             'Natural': SSM(tree_original.levels_nodes[l],\n",
    "                                             tree_Natural_c.levels_nodes[l]),\n",
    "             'OPTICS': SSM(tree_original.levels_nodes[l],\n",
    "                                            tree_OPTICS.levels_nodes[l]),\n",
    "             'knee': SSM(tree_original.levels_nodes[l],\n",
    "                                          tree_knee.levels_nodes[l])\n",
    "             }\n",
    "        resultado_form_metric.append(d)\n",
    "    \n",
    "\n",
    "    return {'Point_dataframes':{'original_retag':data_fram_or,\n",
    "            'Natural_C':df_Natural ,\n",
    "            'DBSCAN':df_DBSCAN,\n",
    "            'HDBSCAN':df_HDBSCAN,\n",
    "            'OPTICS':df_OPTICS,\n",
    "            'knee':df_knee\n",
    "           }, 'metric_form': resultado_form_metric,\n",
    "            'Noise_signal':{\n",
    "            'original_retag':data_fram_or_sig_noise,\n",
    "            'Natural_C':df_Natural_sig_noise ,\n",
    "            'DBSCAN':df_DBSCAN_sig_noise,\n",
    "            'HDBSCAN':df_HDBSCAN_sig_noise,\n",
    "            'OPTICS':df_OPTICS_sig_noise,\n",
    "            'knee':df_knee_sig_noise\n",
    "            }\n",
    "           \n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61e708eb-d16b-4ad2-b21c-459ca8151248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"generate_tree_clusterize_form\" class=\"doc_header\"><code>generate_tree_clusterize_form</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>generate_tree_clusterize_form</code>(**\\*\\*`kwargs`**)\n",
       "\n",
       "Generates all the experiment all the experiment creates the data and clusterize using the algorithm available\n",
       "\n",
       ":param levels_tree: Levels for the tree\n",
       "\n",
       ":param int per_cluster: Points per clusters\n",
       "\n",
       ":param levels_cluster:  Levels to clusterize\n",
       "\n",
       ":param bool verbose:  To print some outputs \n",
       "\n",
       ":returns: a dictionary with all the  data frames and a dictionary \n",
       "with the similarity measurment created\n",
       " "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(generate_tree_clusterize_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c1d1c1f-2de6-47c3-8d65-28d38ae9c251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"get_tree_from_clustering\" class=\"doc_header\"><code>get_tree_from_clustering</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>get_tree_from_clustering</code>(**`cluster_tree_clusters`**)\n",
       "\n",
       "Returns the tree from the iterative clustering, the cluster_tree_cluster\n",
       "\n",
       ":param cluster_tree_clusters is a list of list with a dictionary that \n",
       "       should contain the point of the next level clusters, the name of the parent\n",
       "       cluster of such clusters (name of the current node), and the point that \n",
       "       are consider noise. \n",
       "\n",
       ":return A list of list that contains the nodes for each level of the tree. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_tree_from_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24258920-1730-4e6e-8cd1-225f2e648f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"SSM\" class=\"doc_header\"><code>SSM</code><a href=\"__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>SSM</code>(**`list_poly_c_1`**, **`list_poly_c_2`**, **\\*\\*`kwargs`**)\n",
       "\n",
       "The function calculates the Similarity Form Measurment (SMF)\n",
       "between two clusterizations \n",
       "\n",
       ":param: list of nodes with points and polygons \n",
       "\n",
       ":param: list of nodes with points and polygons \n",
       "\n",
       ":param bool verbose: To print to debugg\n",
       "\n",
       ":returns double: The similarity mesuarment"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(SSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0c4b7-1fee-4144-8325-b6b170ea4366",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "These functions are use by the clustering algorithms and to create the structure, mosly by handly the names for each node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "726e2aab-1813-4071-b0df-f220810ea19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"get_alpha_shape\" class=\"doc_header\"><code>get_alpha_shape</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>get_alpha_shape</code>(**`point_list`**)\n",
       "\n",
       "Returns a polygon representing the hull of the points sample.\n",
       "\n",
       ":param list point_list: list list of tuples with samples coordinates.\n",
       "\n",
       ":returns shapely.Polygon: concave hull shapely polygon"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_alpha_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "397b1e75-e076-493b-8e00-9d57db72e4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"set_colinear\" class=\"doc_header\"><code>set_colinear</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>set_colinear</code>(**`list_points`**)\n",
       "\n",
       "Check if in the list of points any of triplet of points\n",
       "is colinear\n",
       ":param list list_points: List of shapely Points\n",
       "\n",
       ":returns bool: True if all are not colinear "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(set_colinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca55d0d9-9d04-40e1-8a3c-3a84d43df98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"collinear\" class=\"doc_header\"><code>collinear</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>collinear</code>(**`p1`**, **`p2`**, **`p3`**)\n",
       "\n",
       "Check if the points are colinear \n",
       "\n",
       ":param shapely Point p1: point to chek if is colinear\n",
       "\n",
       ":param shapely Point p2: point to chek if is colinear\n",
       "\n",
       ":param shapely Point p3: point to chek if is colinear\n",
       "\n",
       ":return bool: True if are colinear"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(collinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81270750-4c9e-4a99-94d2-6a64806b7232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"get_segments\" class=\"doc_header\"><code>get_segments</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>get_segments</code>(**`points`**)\n",
       "\n",
       "Get the segments from a delaunay triangulation\n",
       "\n",
       ":param points: Point to get Delaunay triangulation and exctract points \n",
       "\n",
       ":return edges: "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fbc96d1-077d-4c02-b5b3-a745d1b4a096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"get_polygons_buf\" class=\"doc_header\"><code>get_polygons_buf</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>get_polygons_buf</code>(**`lines`**)\n",
       "\n",
       "Obtain the poligons from the lines\n",
       "\n",
       ":param list lines: List of lines\n",
       "\n",
       ":returns shapely polygon: the union of the union of \n",
       "edges (Polygon or multypolygon)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(get_polygons_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c7c8064-c68c-40b3-a790-5f48d87950ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"labels_filtra\" class=\"doc_header\"><code>labels_filtra</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>labels_filtra</code>(**`point_points`**, **`multy_pol`**)\n",
       "\n",
       "Labels the points in the multy_pol if no polygon contains \n",
       "a point is label as -1\n",
       "\n",
       ":param shapely MultyPoint point_points: Points to check \n",
       "\n",
       ":param multy_pol\n",
       "\n",
       ":returns np.array: Label array with -1 if is not contained \n",
       "in a polygon"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(labels_filtra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b24e260-03ec-4062-a606-77c313efba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random to use:  332\n",
      "With 2  levels\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Partition1/Cgeo/git/HierarchicalGeoClustering/HierarchicalGeoClustering/TreeClusters.py:96: ShapelyDeprecationWarning: The 'cascaded_union()' function is deprecated. Use 'unary_union()' instead.\n",
      "  return shapely.ops.cascaded_union(keep_triangles)\n",
      "/Partition1/Cgeo/git/HierarchicalGeoClustering/HierarchicalGeoClustering/TreeClusters.py:377: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  points_cluster_o = [p for p in points_cluster if\n",
      "/Partition1/Cgeo/git/HierarchicalGeoClustering/HierarchicalGeoClustering/TreeClusters.py:1086: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  all_points =all_points+[i for i in node.get_points(all_tag = iterative )]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1404\n",
      "tree with too few elements to clusterize creating new tree\n",
      "Random to use:  1372\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "###Test\n",
    "generate_tree_clusterize_form(tree_level= 2,per_cluster = 100, levels_cluster =  2,verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f3342-26f3-474b-b899-3b47286edf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
